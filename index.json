[{"content":" Latest Blog Posts and Articles on Linux, Container technologies, Kubernetes, and Personal experiences. ","date":null,"permalink":"/blog/","section":"BLOG","summary":"","title":"BLOG"},{"content":"","date":null,"permalink":"/tags/containerization/","section":"TOPICS","summary":"","title":"Containerization"},{"content":"","date":null,"permalink":"/tags/containers/","section":"TOPICS","summary":"","title":"Containers"},{"content":"This article is Part II of the previously published article, Containers from Scratch: Deep Dive into Single-Host Container Networking. In Part I, you\u0026rsquo;ve learned how two containers communicate on the same single host, also known as Single-host container networking.\nIn this article, Part II, you\u0026rsquo;ll learn how Containers (Container A and Container B) running on two different hosts (VMs) communicate and interact with each other using VXLAN networking, also known as Multi-host container networking, and I\u0026rsquo;ll also demonstrate how Multi-host container networking works at the underlying layer with built-in Linux command-line tools.\nSummary: Objectives #What you\u0026rsquo;ll learn in this article:\nBasic Multi-Host Networking concepts\nBasic Concepts of Kubernetes CNI (Container Network Interface) plugins\nWhat\u0026rsquo;s VXLAN (Virtual eXtensible Local-Area Network) and how it works\nHow Containers running on Different Hosts communicate and interact using VXLAN networking\nPrerequisites #Before you begin, make sure you\u0026rsquo;ve installed the following tools:\nLinux-based Two VMs (or) Servers Basic Networking Concepts Familiar with Linux networking tools, such as ip and brctl Overview of Multi-host Container Networking #You\u0026rsquo;ve previously learned how two containers communicate on the same single host, only one host (or) VM, also known as Single-host container networking. For example, Docker single host.\nIn Multi-host container networking facilitated by overlay networks, containers running on the different hosts (or) VMs can communicate with each other in the same network. Sometimes, we refer to it as Multi-node (or) Cluster networking. For example, Docker Swarm mode, Kubernetes.\nTechnically, we can use two different networking methods for Multi-host container networking.\nVXLAN (Virtual eXtensible Local-Area Network) is a tunneling protocol or network virtualization technology that provides tunneling a virtual Layer 2 network (overlay network) over the Layer 3 network (underlay network).\nDirect Routing (also known as Native or Simple Routing) is the Linux kernel\u0026rsquo;s built-in capabilities for forwarding network packets between different networks, rather than using specific software or network protocols. That means it\u0026rsquo;s no encapsulation, no overlay network. It\u0026rsquo;s just simple IP routing.\nFor Example, the following Kubernetes CNI (Container Network Interface) plugins use different networking modes. But some Kubernetes CNIs provide both VXLAN (Encapsulation) and Direct Routing (or Native Routing) modes.\nFlannel: VXLAN is the default networking mode of Flannel.\nCalico: Calico\u0026rsquo;s default networking mode is BGP (Border Gateway Protocol) or Direct Routing, but you can also use the VXLAN networking mode.\nCilium: Cilium also provides both VXLAN and Direct Routing (or Native Routing) modes. But the default networking mode is VXLAN or tunnel mode, also known as encapsulation mode.\nKube-router: The default networking mode of Kube-router is BGP or Direct Routing (or Native Routing) as the main routing mechanism and so on.\nYou can see the CNI full list on https://github.com/containernetworking/cni?tab=readme-ov-file#3rd-party-plugins.\nIn this article, I will focus on VXLAN Networking to demonstrate Multi-host container networking from scratch.\nWhat\u0026rsquo;s VXLAN and How it Works # Photo Credit to: RedHat Developers (developers.redhat.com)\nVXLAN (Virtual eXtensible Local-Area Network) is a tunneling protocol or network virtualization technology that provides for creating a virtual Layer 2 network (overlay network) over the Layer 3 network (underlay network).\nAn Overlay Network is a virtual or logical network built on top of an existing physical network (also known as underlay network). It provides services, such as network virtualization, segmentation, and tunneling. For examples; VPNs, VXLAN.\nAn Underlay Network is the physical network infrastructure that provides the actual connectivity. For example; network routers, switches.\nVXLAN encapsulates the Layer 2 Ethernet frames into UDP packets. This enables Layer 2 network (the data link layer, e.g: switch) traffic to traverse a Layer 3 network (the network layer, e.g; router and IP address). It\u0026rsquo;s especially used in data centers, cloud environments, scalable overlay networks for VMs, and containers.\nUse Cases #There are example use cases of VXLAN:\nCloud Networking — Connecting VMs across hosts. Container Networking — Kubernetes CNI (Container Network Interface) plugins. VXLAN Components #There are key components of VXLAN:\nVXLAN Tunnel Endpoint (VTEP): This is the core component that performs the encapsulation and decapsulation of VXLAN packets. VTEPs can be physical network devices or virtual switches within hypervisors (e.g: VMware). Each VTEP has a unique IP address in the underlay network.\nVXLAN Network Identifier (VNI): This is a 24-bit identifier that uniquely identifies each virtual network segment within the VXLAN overlay.\nUnderlay Network: This is the physical network that VXLAN traffic traverses. It provides the routing infrastructure for the encapsulated VXLAN packets.\nOverlay Network: This is the virtual network created by VXLAN, running on top of the underlay physical network. It allows VMs (or Servers) to communicate.\nHow VXLAN Works #Basically, VXLAN (Virtual eXtensible Local-Area Network) works by encapsulating L2 Ethernet frames in UDP/IP with VTEPs handling the mapping between virtual overlay and physical underlay networks.\nSimple Usage:\n$ ip link add \u0026lt;vx0\u0026gt; type vxlan id 100 local \u0026lt;10.0.0.100\u0026gt; remote \u0026lt;10.0.0.200\u0026gt; dev eth0 dstport 4789 Setup (1): Frame Arrival # A VM or Server sends an Ethernet frame. For example, Host (A) ⟶ Host (B). The frame reaches the local VTEP (VXLAN Tunnel Endpoint). Setup (2): Encapsulation # The VTEP checks the VNI (VXLAN Network Identifier) and destination MAC address. It then encapsulates the frame inside a UDP/IP packet: Outer Source IP: Local VTEP IP 10.0.0.100 Outer Destination IP: Remote VTEP IP 10.0.0.200 VNI: 100 (For example, 100) UDP Port: 4789 (The default VXLAN UDP port is 4789) Setup (3): Underlay Forwarding # The encapsulated packet is sent over the physical (underlay) network. Setup (4): Decapsulation at Remote VTEP # The remote VTEP (10.0.0.200) receives the packet. Then, it checks the following: UDP Port: 4789 ⟶ identifies it as VXLAN. VNI: 100 ⟶ determines which virtual network it belongs to. The ethernet frame is delivered to the correct destination VM or server. Multi-Host Container Networking from Scratch #In this section, I will focus on configuring the network for Two Containers — Container A and Container B, running on Two Different VMs (Hosts) to communicate with each other. Make sure you have two Linux VMs or servers. In this article, I will use two AWS EC2 Instances to demonstrate how Multi-Host Container networking works using the VXLAN networking mode.\nOur project setup looks like this. Container A and Container B are running on Two different hosts.\nContainer A (10.0.0.100) on Host (1) — Debian Linux VM (172.31.89.40) Container B (10.0.0.200) on Host (2) — Amazon Linux VM (172.31.94.69) How it Works # Diagram on How Multi-Host Container Networking Works\nContainer (A) and Container (B) are running on two different hosts (Host 1 and Host 2).\nVETH (Virtual Ethernet) pair veth0,veth1 that connects the network between Host and Container in the same Linux network namespace. veth0 on the Host machine and veth1 on the container.\nVXLAN (Virtual eXtensible Local-Area Network) creates a tunnel that connects Host (1) — Debian Linux VM and Host (2) — Amazon Linux VM.\nThen, the Bridge network br0 is a network switch that forwards network packets between VXLAN and VETH network interfaces. Then, Container (A) and Container (B) can communicate with each other.\nIn the next section, you\u0026rsquo;ll learn how to set up and configure the network in more detail.\nOn Host (1) Debian Linux VM #Configuring Container Network #Firstly, I will set up and configure the network for Container A on Host (1) — Debian Linux. For running Containers, we will use the Alpine Linux root filesystem image.\nMake sure you familiar with how to run a Container from scratch with unshare, chroot tools and you\u0026rsquo;ve learned how to run it in the previous article, Part I — Containers from Scratch: Deep Dive into Single-Host Container Networking.\nCreate a project directory and download the Alpine Linux root filesystem image. This setup is same as the previous (Part I) article.\n$ mkdir -p containers/alpine-linux $ curl -LO https://dl-cdn.alpinelinux.org/alpine/v3.21/releases/x86_64/alpine-minirootfs-3.21.3-x86_64.tar.gz Extract the alpine-minirootfs-3.21.3-x86_64.tar.gz tar file and clean up.\n$ tar -xzvf alpine-minirootfs-3.21.3-x86_64.tar.gz $ rm alpine-minirootfs-3.20.2-x86_64.tar.gz Project structure looks like this:\n~/containers/alpine-linux ├── bin ├── dev ├── etc ├── home ├── lib ├── media ├── mnt ├── opt ├── proc ├── root ├── run ├── sbin ├── srv ├── sys ├── tmp ├── usr └── var I will create and run Container A in an isolated PID (Process ID), mount, and network namespaces using the command-line tools, unshare, chroot.\nThe following command creates a Container (Container A) that is fully PID, mount, and network isolated from the Host (OS) machine and then mounts the /proc virtual filesystem.\n$ cd ~/containers/alpine-linux $ sudo unshare --pid --mount --net \\ -f chroot ./ \\ env -i HOSTNAME=alpine-linux \\ /bin/sh -c \u0026#34;mount -t proc proc /proc; exec /bin/sh;\u0026#34; Then, open another terminal on the your Host machine and get the container process ID with the following command.\nThe following command gets the current running container\u0026rsquo;s PID and sets the environment variable and then I will use this ENV variable when configuring the container network.\n$ export CONTAINER_PID=$(ps -C sh -o pid= | tr -d \u0026#39; \u0026#39;) Then, create a VETH (Virtual Ethernet) network pair veth0, veth1 with the ip command-line tool.\nThe following command creates a VETH pair veth0, veth1 and sets veth1 to the network namespace with the current running container PID and then brings veth0 up. The VETH device is like a local Ethernet tunnel, and a VETH pair consists of two interfaces — one in the host machine\u0026rsquo;s network namespace and another one in the container\u0026rsquo;s network namespace. In the above example, veth0 is in the host machine\u0026rsquo;s Net namespace and veth1 is in the container\u0026rsquo;s Net namesapce.\nsudo ip link add veth0 type veth peer name veth1 sudo ip link set veth1 netns ${CONTAINER_PID} sudo ip link set dev veth0 up Then, set the IP address of Container A by running the following command without entering into the container\u0026rsquo;s shell.\nThe following command sets the IP address 10.0.0.100 to the veth1 network interface, also known as Container A\u0026rsquo;s IP address and brings lo, veth1 up. You can use nsenter to exec commands without entering the container\u0026rsquo;s shell.\nContainer A\u0026rsquo;s IP address ⟶ 10.0.0.100\nsudo nsenter --target ${CONTAINER_PID} \\ --mount \\ --net \\ --pid \\ chroot ${HOME}/containers/alpine-linux \\ /bin/sh -c \u0026#34;ip addr add dev veth1 10.0.0.100/24; ip link set lo up; ip link set veth1 up\u0026#34; Then, I will create a bridge network and attach veth0 to the bridge network with the following command.\nThe following command creates a Bridge network interface named br0, attaches veth0 to the br0 bridge interface, sets the IP address, and brings it up. Make sure you create and configure the bridge network because we need to communicate between the Container and the Host machine. A bridge network is like a network switch that forwards packets between network interfaces that are connected to it.\n$ sudo ip link add br0 type bridge $ sudo ip link set veth0 master br0 $ sudo ip addr add dev br0 10.0.0.1/24 $ sudo ip link set br0 up Configuring VXLAN Network Interface #In this section, I will set up and configure VXLAN to create a tunnel between Host (1) — Debian Linux VM and Host (2) — Amazon Linux VM, and then you can communicate between Container A and Container B.\nOn the Host (1) Debian Linux VM, create a VXLAN interface with the ip command-line tool.\nThe following command creates a VXLAN interface named vxlan0 and brings it up. It creates a tunnel and connects two VMs or servers. Host (1) — Debian Linux VM (172.31.89.40) is local and Host (2) — Amazon Linux VM (172.31.94.69) is remote. Make sure you set the same VNI (id) on both Host (1) and Host (2).\nLocal IP Address ⟶ 172.31.89.40 (Debian Linux VM) Remote IP Address ⟶ 172.31.94.69 (Amazon Linux VM) VXLAN Network Identifier (VNI) ⟶ 100 Destination Port⟶ 4789 (Default UDP Port) Network Interface (Device) ⟶ enX0 (Ethernet network device on the Debian Linux VM) sudo ip link add vxlan0 \\ type vxlan \\ id 100 \\ local 172.31.89.40 \\ remote 172.31.94.69 \\ dstport 4789 \\ dev enX0 \u0026amp;\u0026amp; \\ sudo ip link set vxlan0 up Then, attach the vxlan0 interface to the bridge network device with the following command.\nThe following command attaches the vxlan0 interface to the br0 bridge network device. Previously, we\u0026rsquo;ve created this bridge device and make sure you attach your VXLAN interface to the bridge network device. We need to forward packets or connect the VETH and VXLAN interfaces because it\u0026rsquo;s necessary to communicate between Container A (running on Host 1) and Container B (running on Host 2).\nsudo ip link set vxlan0 master br0 Then, you can check it with the brctl command-line tool. Make sure your veth0 and vxlan0 are attached to the bridge br0 device.\n$ brctl show bridge name bridge id STP enabled interfaces br0 8000.8aea1d11531b no veth0 vxlan0 On Host (2) Amazon Linux VM #Configuring Container Network #Same as the previous Host (1) setup, I will set up and configure the network for Container B on Host (2) — Amazon Linux. To create and run the container, we will use the Alpine Linux root filesystem image.\nCreate a project directory and download the Alpine Linux root filesystem image. This setup is same as the previous (Part I) article.\n$ mkdir -p containers/alpine-linux $ curl -LO https://dl-cdn.alpinelinux.org/alpine/v3.21/releases/x86_64/alpine-minirootfs-3.21.3-x86_64.tar.gz Extract the alpine-minirootfs-3.21.3-x86_64.tar.gz tar file and clean up.\n$ tar -xzvf alpine-minirootfs-3.21.3-x86_64.tar.gz $ rm alpine-minirootfs-3.20.2-x86_64.tar.gz Project structure looks like this:\n~/containers/alpine-linux ├── bin ├── dev ├── etc ├── home ├── lib ├── media ├── mnt ├── opt ├── proc ├── root ├── run ├── sbin ├── srv ├── sys ├── tmp ├── usr └── var I will create and run Container B in an isolated PID (Process ID), mount, and network namespaces using the command-line tools, unshare, chroot.\nThis command creates a Container (Container B) that is fully PID, mount, and network isolated from the Host (OS) machine and then mounts the /proc virtual filesystem.\n$ cd ~/containers/alpine-linux $ sudo unshare --pid --mount --net \\ -f chroot ./ \\ env -i HOSTNAME=alpine-linux \\ /bin/sh -c \u0026#34;mount -t proc proc /proc; exec /bin/sh;\u0026#34; Then, open another terminal on the your Host machine and get the container process ID with the following command.\nThis command gets the current running container\u0026rsquo;s PID and sets the environment variable and then I will use this ENV variable when configuring the container network.\n$ export CONTAINER_PID=$(ps -C sh -o pid= | tr -d \u0026#39; \u0026#39;) Then, create a VETH (Virtual Ethernet) network pair veth0, veth1 with the ip command-line tool.\nThis command creates a VETH pair veth0, veth1 and sets veth1 to the network namespace with the current running container PID and then brings veth0 up. The VETH device is like a local Ethernet tunnel, and a VETH pair consists of two interfaces — one in the host machine\u0026rsquo;s network namespace and another one in the container\u0026rsquo;s network namespace. In the above example, veth0 is in the host machine\u0026rsquo;s Net namespace and veth1 is in the container\u0026rsquo;s Net namesapce.\nsudo ip link add veth0 type veth peer name veth1 sudo ip link set veth1 netns ${CONTAINER_PID} sudo ip link set dev veth0 up Then, set the IP address of Container B by running the following command without entering into the container\u0026rsquo;s shell.\nThis command sets the IP address 10.0.0.200 to the veth1 network interface, also known as Container B\u0026rsquo;s IP address and brings lo, veth1 up. You can use nsenter to exec commands without entering the container\u0026rsquo;s shell.\nContainer B\u0026rsquo;s IP address ⟶ 10.0.0.200\nsudo nsenter --target ${CONTAINER_PID} \\ --mount \\ --net \\ --pid \\ chroot ${HOME}/containers/alpine-linux \\ /bin/sh -c \u0026#34;ip addr add dev veth1 10.0.0.200/24; ip link set lo up; ip link set veth1 up\u0026#34; Then, I will create a bridge network and attach veth0 to the bridge network with the following command.\nThis command creates a Bridge network interface named br0, attaches veth0 to the br0 bridge interface, sets the IP address, and brings it up. Make sure you create and configure the bridge network because we need to communicate between the Container and the Host machine. A bridge network is like a network switch that forwards packets between network interfaces that are connected to it.\n$ sudo ip link add br0 type bridge $ sudo ip link set veth0 master br0 $ sudo ip addr add dev br0 10.0.0.2/24 $ sudo ip link set br0 up Configuring VXLAN Network Interface #In this section, I will set up and configure VXLAN to create a tunnel between Host (1) — Debian Linux VM and Host (2) — Amazon Linux VM, and then you can communicate between Container A and Container B.\nSame as the previous Host (1) setup, on the Host (2) Amazon Linux VM, create a VXLAN interface with the ip command-line tool.\nThis command creates a VXLAN interface named vxlan0 and brings it up. It creates a tunnel and connects two VMs or servers. In this setup, Host (2) — Amazon Linux VM (172.31.94.69) is local and Host (1) — Debian Linux VM (172.31.89.40) is remote. Make sure you set the same VNI (id) on both Host (1) and Host (2).\nLocal IP Address ⟶ 172.31.94.69 (Host 2 Amazon Linux VM) Remote IP Address ⟶ 172.31.89.40 (Host 1 Debian Linux VM) VXLAN Network Identifier (VNI) ⟶ 100 Destination Port ⟶ 4789 (Default UDP Port) Network Interface (Device) ⟶ enX0 (Ethernet network device on the Host 2 Amazon Linux VM) sudo ip link add vxlan0 \\ type vxlan \\ id 100 \\ local 172.31.94.69 \\ remote 172.31.89.40 \\ dstport 4789 \\ dev enX0 \u0026amp;\u0026amp; \\ sudo ip link set vxlan0 up Then, attach the vxlan0 interface to the bridge network device with the following command.\nThis command attaches the vxlan0 interface to the br0 bridge network device. Previously, we\u0026rsquo;ve created this bridge device and make sure you attach your VXLAN interface to the bridge network device. We need to forward packets or connect the VETH and VXLAN interfaces because it\u0026rsquo;s necessary to communicate between Container A (running on Host 1) and Container B (running on Host 2).\nsudo ip link set vxlan0 master br0 Then, you can check it with the brctl command-line tool. Make sure your veth0 and vxlan0 are attached to the bridge br0 device.\n$ brctl show bridge name bridge id STP enabled interfaces br0 8000.8aea1d11531b no veth0 vxlan0 Testing Network Connectivity #Now, you\u0026rsquo;ve configured the container network and VXLAN tunnel, and you can ping the containers\u0026rsquo; IP addresses to confirm if it works.\nPing Container A (10.0.0.100) from the Host 2 (Amazon Linux VM).\nping 10.0.0.100 Ping Container B (10.0.0.200) from the Host 1 (Debian Linux VM).\nping 10.0.0.200 Reference Links:\nhttps://blog.mbrt.dev/posts/container-network https://labs.iximiuz.com/tutorials/container-networking-from-scratch https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking ","date":"25 May 2025","permalink":"/blog/deep-multi-host-container-networking/","section":"BLOG","summary":"How Containers running on different Hosts communicate at the underlying layer","title":"Containers from Scratch: Deep Dive into Multi-Host Container Networking (Part II)"},{"content":"","date":null,"permalink":"/tags/networking/","section":"TOPICS","summary":"","title":"Networking"},{"content":"","date":null,"permalink":"/tags/","section":"TOPICS","summary":"","title":"TOPICS"},{"content":" Welcome to the ZawZaw.blog. Explains and publishes in-depth/practical guides on Linux, Container technologies, Kubernetes, and Personal experiences. Read more in the Blog section. ","date":null,"permalink":"/","section":"Welcome to ZawZaw.blog","summary":"","title":"Welcome to ZawZaw.blog"},{"content":"This article focused on a deep dive into Container networking; how to run Containers and configure Container Networking from scratch using the tools, such as Linux Namespaces, chroot, unshare and ip. This article also provides a hands-on practical guide on how to run and configure from scratch using these tools. You\u0026rsquo;ll mainly learn how Container Networking works at the underlying layer (or low level), and then you\u0026rsquo;ll clearly understand how Docker Container Networking works.\nSummary: Objectives #What you\u0026rsquo;ll learn in this article:\nBasic Concepts of Containerization.\nLinux Namespaces, the foundation of modern Container technologies.\nHow Linux Namespaces work and How Containers isolate from the Host OS.\nHow Virtual Ethernet and Bridge networking work.\nHow Containers and Container Networking work at the underlying layer, also known as low level.\nPrerequisities #Before we begin, make sure you are familiar with the following tools:\nLinux Host (e.g., Ubuntu, Fedora, etc..)\nchroot ─ a user-space tool to interact with the chroot(2) system call, to change the root filesystem of the calling process.\nunshare ─ a user-space tool to interact with Linux kernel namespaces by invoking the unshare(2) system call, to create a new process in a new namespace that isolates the process ID, mount, IPC, network, and so on.\nip ─ a command-line tool to configure network interfaces, routing, and tunnels.\nLinux namespaces ─ a feature of the Linux kernel that isolates and virtualizes system resources for a collection of processes.\nNOTE: If you have installed any Linux distribution, these tools are built-in tools and features.\nIntroduction to Containers # Docker Containers vs Virtual Machines by Docker\nNowadays, containers are a popular topic, and most companies are using containers to build, ship and run application workloads in both development and production environments.\nBasically, containers are a way to package and deploy applications in an isolated and portal environment and they provide a standardized way to bundle an application\u0026rsquo;s code, dependencies and configuration into a single unit that can be easily deployed on the server.\nContainers use OS-level virtualization to create an isolated environment for running applications. OS-level virtualization is a technology that allows multiple isolated Operating Systems to run on the same hardware, also known as containerization. Containers share the Host OS\u0026rsquo;s kernel and hardware resources, such as CPU and memory that make resource efficiency.\nBenefits of using Containers are:\nPortability: Containers can be moved easily between different environments, such as development, QA, staging and production.\nConsistency: Containers ensure that the application and its dependencies are packaged into a single unit that can be deployed easily.\nScalability: Containers can be scaled up or down easily on demand.\nEfficiency: Containers are lightweight and share the Host OS\u0026rsquo;s kernel and hardware resources that make more efficient to run applications.\nSetup Project and Root Filesystem #Firstly, we will build and run Containers from scratch using the chroot and unshare command-line tools to understand how Containers work.\nProject Structure looks like:\n${HOME}/containers ├── alpine-linux │ ├── bin │ ├── dev │ ├── etc │ ├── home │ ├── lib │ ├── media │ ├── mnt │ ├── opt │ ├── proc │ ├── root │ ├── run │ ├── sbin │ ├── srv │ ├── sys │ ├── tmp │ ├── usr │ └── var └── tiny-linux ├── bin ├── dev ├── proc ├── sbin ├── sys └── usr Create a project directory named, containers and then, we will put the Linux root filesystems (Alpine Linux, Tiny Linux) into it.\n$ mkdir -p containers/alpine-linux In this article, we will use the Alpine Linux Mini root filesystem. Go to the Alpine Linux official website https://alpinelinux.org/downloads and download the Alpine Linux Mini root filesystem.\nAlpine Linux has supported the Alpine Mini root filesystem that is for containers and minimal chroots. That supports multiple system architectures, such as aarch64, armv7. riscv64, x86, x86_64 and so on.\n(Or)\nDownload with the curl command line tool. For example, x86_64 architecture.\n$ curl -LO https://dl-cdn.alpinelinux.org/alpine/v3.21/releases/x86_64/alpine-minirootfs-3.21.3-x86_64.tar.gz Put the downloaded Alpine Linux Mini rootfs file under the containers/alpine-linux directory and then, extract the mini rootfs tar file.\n$ tar -xzvf alpine-minirootfs-3.21.3-x86_64.tar.gz Then, clean up the alpine-minirootfs-3.21.3-x86_64.tar.gz tar file.\n$ rm alpine-minirootfs-3.20.2-x86_64.tar.gz Running Containers from Scratch #Chroot: Basic Concept of Containers #/ (Host Root Filesystem) ├── bin ├── dev ├── etc ├── home │ └── zawzaw/ │ └── containers/ │ ├── alpine-linux/ │ │ ├── bin │ │ ├── dev │ │ ├── etc │ │ ├── home │ │ ├── lib │ │ ├── proc │ │ ├── sbin │ │ └── var │ └── tiny-linux/ │ ├── bin │ ├── dev │ ├── init.sh │ ├── linuxrc -\u0026gt; bin/busybox │ ├── proc │ ├── sbin │ ├── sys │ ├── sbin │ └── usr ├── proc ├── sbin ├── sys ├── usr └── var chroot (Change Root) is a Basic Concept of Containers. chroot is an operation that changes the apparent root directory for the current running process and its children on Unix and Unix-like operating systems. Historically, the chroot system call was introduced in Unix Seventh Edition (Version 7) in 1979.\nThe chroot user-space program (or cleint command-line tool) functionality relies on kernel support because it calls the system call chroot(2) handled by the kernel. This means that while you can execute chroot in user space, its effects depend on kernel-level enforcement.\nchroot is a user-space program (or) client command-line tool. It calls the chroot(2) system call, which is handled by the kernel. It is commonly used for sandboxing processes (or) creating minimal environments for recovery and testing. A process inside chroot still runs with the same privileges (it does not enhance security like containers do). Using the chroot User-space Tool #On Linux, we can use the chroot client command-line tool, to interact with the chroot(2) system call (a kernel API function call) to change the root directory for the current running process.\nGo to the already created project directory ${HOME}/containers/alpine-linux and run the chroot command.\n$ cd $HOME/containers/alpine-linux $ sudo chroot . /bin/sh / # ls -l total 0 drwxr-xr-x 1 1000 1000 858 Jul 22 14:34 bin drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 dev drwxr-xr-x 1 1000 1000 540 Jul 22 14:34 etc drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 home drwxr-xr-x 1 1000 1000 272 Jul 22 14:34 lib drwxr-xr-x 1 1000 1000 28 Jul 22 14:34 media drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 mnt drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 opt dr-xr-xr-x 1 1000 1000 0 Jul 22 14:34 proc drwx------ 1 1000 1000 24 Jul 30 04:26 root drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 run drwxr-xr-x 1 1000 1000 790 Jul 22 14:34 sbin drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 srv drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 sys drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 tmp drwxr-xr-x 1 1000 1000 40 Jul 22 14:34 usr drwxr-xr-x 1 1000 1000 86 Jul 22 14:34 var / # cat /etc/os-release NAME=\u0026#34;Alpine Linux\u0026#34; ID=alpine VERSION_ID=3.21.3 PRETTY_NAME=\u0026#34;Alpine Linux v3.21\u0026#34; HOME_URL=\u0026#34;https://alpinelinux.org/\u0026#34; BUG_REPORT_URL=\u0026#34;https://gitlab.alpinelinux.org/alpine/aports/-/issues\u0026#34; Then, you need to mount the proc virtual filesystem inside the chroot isolated environment.\n$ mount -t proc proc /proc This mount command mounts the /proc Virtual Filesystem (VFS) to the /proc directory inside the chroot (or) container environment.\nmount: This command to attach a filesystem to the directory tree. -t proc: Specifies the filesystem type as the proc virtual filesystem. proc: The source since proc is virtual, no physical device is used. /proc: The mount point where the filesystem will be attached in the chroot isolated root filesystem. Why do we need to mount the /proc filesystem?\nMounting the /proc filesystem inside the chroot environment is necessary because the /proc virtual filesystem is a critical component (or) feature that is information about the system and process provided by the Linux kernel to the user-space apps and tools. Without it, many user-space apps and tools (e.g; ps, top) that rely on /proc will not work properly.\nFor example, the ps tool will not work properly.\n$ ps aux Output:\n# Error: Could not read /proc/stat Read more details about the /proc virtual filesystem in the next section.\nThe /proc Virtual Filesystem #The proc filesystem (often referred to as /proc) is a Virtual Filesystem (VFS) also known as a Pseudo (or) Special Filesystem on Linux that provides a way to expose system and process information to users and user-space applications in a structured, file-like format by the Linux kernel.\nThat does not rely on physical storage devices, such as HDDs and SSDs. The files and directories in /proc are not stored on disk and exist only in memory, and are generated on-the-fly (or) exposed dynamically by the Linux kernel when the system is booted.\nKey Features of the /proc Filesystem:\n(1) Virtual and Dynamic:\nThe files and directories in /proc are generated dynamically by the Linux kernel. The files and directories in /proc don\u0026rsquo;t exist on disk; they are created in memory when read. (2) System and Process Information:\n/proc provides the detailed information about: Running processes (e.g; /proc/[PID] for each process). System hardware (e.g; CPU: /proc/cpuinfo, Memory: cat /proc/meminfo) Kernel configuration and runtime parameters. (3) Readable and Writable:\nMost files in /proc are readable (e.g; you can cat /proc/cpuinfo). Some files are writable, allowing you to modify kernel parameters at runtime (e.g; /proc/sys). After you run the mount -t proc proc /proc command inside the container, you can test the following commands.\ncat /etc/os-release: To check the running container\u0026rsquo;s Linux distribution.\ncat /proc/version: To check the Linux kernel version that is shared from the Host OS.\ncat /proc/cpuinfo: To check the CPU information that is shared from the Host OS.\ncat /proc/meminfo: To check the Memory information that is shared from the Host OS.\nps aux: To check all processes that are running inside the container.\nip addr show: To see all IP addresses inside the currently running container.\nFor example,\n/ # cat /etc/os-release NAME=\u0026#34;Alpine Linux\u0026#34; ID=alpine VERSION_ID=3.21.3 PRETTY_NAME=\u0026#34;Alpine Linux v3.21\u0026#34; HOME_URL=\u0026#34;https://alpinelinux.org/\u0026#34; BUG_REPORT_URL=\u0026#34;https://gitlab.alpinelinux.org/alpine/aports/-/issues\u0026#34; / # cat /proc/version Linux version 6.13.5-200.fc41.x86_64 (mockbuild@be03da54f8364b379359fe70f52a8f23) (gcc (GCC) 14.2.1 20250110 (Red Hat 14.2.1-7), GNU ld version 2.43.1-5.fc41) #1 SMP PREEMPT_DYNAMIC Thu Feb 27 15:07:31 UTC 2025 / # cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 142 model name : Intel(R) Core(TM) i7-8565U CPU @ 1.80GHz stepping : 12 microcode : 0xfc cpu MHz : 2900.231 cache size : 8192 KB ... / # cat /proc/meminfo MemTotal: 16210104 kB MemFree: 3241444 kB MemAvailable: 9901900 kB Buffers: 5496 kB Cached: 7888196 kB ... Then, you will notice that the Alpine Linux container is using the Host OS kernel, Fedora Linux, with the kernel version 6.13.5-200.fc41.x86_64 that is shared from the Host OS. And also shared CPU and Memory from the Host machine.\nThen, we wil test the currently running processes and IP addresses inside the container like this.\n/ # ps aux PID USER TIME COMMAND 1 root 0:05 /usr/lib/systemd/systemd --switched-root --system --deserialize=51 rhgb 2 root 0:00 [kthreadd] 3 root 0:00 [pool_workqueue_] 4 root 0:00 [kworker/R-rcu_g] 5 root 0:00 [kworker/R-sync_] 6 root 0:00 [kworker/R-slub_] 7 root 0:00 [kworker/R-netns] 9 root 0:00 [kworker/0:0H-ev] 12 root 0:00 [kworker/R-mm_pe] 14 root 0:00 [rcu_tasks_kthre] 15 root 0:00 [rcu_tasks_rude_] 16 root 0:00 [rcu_tasks_trace] 17 root 0:16 [ksoftirqd/0] 18 root 0:10 [rcu_preempt] 19 root 0:00 [rcu_exp_par_gp_] 20 root 0:00 [rcu_exp_gp_kthr] ... / # ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host noprefixroute valid_lft forever preferred_lft forever 2: wlo1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP qlen 1000 link/ether fa:8b:5b:09:59:51 brd ff:ff:ff:ff:ff:ff inet 192.168.55.127/24 brd 192.168.55.255 scope global dynamic noprefixroute wlo1 valid_lft 75600sec preferred_lft 75600sec inet6 fe80::fa24:a316:4e60:c881/64 scope link noprefixroute valid_lft forever preferred_lft forever ... In that time, you will notice that we will see all processes and network interfaces of the Host OS, Fedora Linux, from the chroot Alpine Linux container. It means that we have not isolated the process ID (PID) and network.\nIn the next section, we will learn how Linux namespaces work and how to use the unshare user-space client tool to interact with Linux namespaces to start a process in a namespace that isolates the process ID (PID), mount, IPC, network, and so on.\nLinux Namespaces #Namespaces are a feature of the Linux kernel that isolates and virtualizes system resources for a collection of processes. Namespaces have been released in the Linux kernel version 2.4.19 since 2002.\nNamespaces are the foundation of modern Containerization technologies, such as Docker and Podman. Namespaces enable multiple processes to have different views of the system, such as different process IDs, network interfaces, filesystems, and so on.\nDocumentation: https://man7.org/linux/man-pages/man7/namespaces.7.html\nTypes of Linux Namespaces:\nNamespace Manual Page Isolates PID (Process ID) pid_namespaces Isolates process IDs. Mount mount_namespaces Isolates the set of mounted filesystems. UTS (UNIX Timesharing System) uts_namespaces Isolates hostname and DNS name. IPC (Inter-process Communication) ipc_namespaces Isolates IPC resources, such as message queue and shared memory. Network network_namespaces Isolates network interfaces, IP addresses, routing tables, and port numbers. User user_namespaces Isolates user and group IDs. CGroup cgroup_namespaces Isolates the view of Control Groups (CGroups). How Linux Namespaces Work #The Namespaces API supported the following system calls and Namespaces are created using these system calls.\nclone() Creates a new process in a new namespace.\nunshare() Creates (or) moves the calling process to a new namespace.\nsetns() Allows a process to join an existing namespace.\nExample Use Cases\nConsider a Container running on a Linux Host machine:\nThe container has its own Mount namespaces, So it can have its own root filesystem that is isolated from the Host OS.\nThe container has its own UTS namespaces, So it can have its own hostname that is isolated from the Host OS.\nThe container has its own PID namespaces, So processes inside the container have its own PIDs that are isolated from the Host OS.\nThe container has its own Network namespaces, So it can have its own IP addresses and network interfaces that are isolated from the Host OS.\nUsing the unshare User-space Tool #The unshare command-line tool is a user-space tool to interact with Linux kernel namespaces by invoking the unshare(2) system call, to create a new process in a new namespace (or) move a process into an existing namespace that isolates the process IDs, mount points, IPC, network interfaces, and so on.\nThe unshare CLI client tool creates new namespaces for the calling process and it can create one or more of the following namespaces.\nPID (Process ID) Mount UTS (Hostname and domain name) IPC (Inter-process Communication) Network User Cgroup Command Options:\n--mount: Create a new mount namespace.\n--uts: Create a new UTS namespace (isolates hostname and domain name).\n--ipc: Create a new IPC namespace.\n--net: Create a new network namespace.\n--pid: Create a new PID namespace.\n--user: Create a new user namespace.\n--cgroup: Create a new cgroup namespace.\n--fork: Fork a new process to run the command (required for PID namespaces).\nGo to the project directory,\n$ cd containers/alpine-linux Then, we will create a process, the Alpine Linux container, in PID (Process ID), Mount and Network namespaces using the unshare and chroot CLI tools.\n$ sudo unshare --pid --mount --net -f chroot ./ /bin/sh / # ls -l total 4 drwxr-xr-x 1 1000 1000 858 Feb 13 23:04 bin drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 dev drwxr-xr-x 1 1000 1000 540 Feb 13 23:04 etc drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 home drwxr-xr-x 1 1000 1000 146 Feb 13 23:04 lib drwxr-xr-x 1 1000 1000 28 Feb 13 23:04 media drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 mnt drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 opt dr-xr-xr-x 1 1000 1000 0 Feb 13 23:04 proc drwx------ 1 1000 1000 24 Mar 12 09:10 root drwxr-xr-x 1 1000 1000 8 Feb 13 23:04 run drwxr-xr-x 1 1000 1000 790 Feb 13 23:04 sbin drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 srv drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 sys drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 tmp drwxr-xr-x 1 1000 1000 40 Feb 13 23:04 usr drwxr-xr-x 1 1000 1000 86 Feb 13 23:04 var Then, mount the /proc virtual filesystem to see the system and process information.\n$ mount -t proc proc /proc Then, check running processes and network interfaces on the Alpine Linux container.\n/ # ps aux PID USER TIME COMMAND 1 root 0:00 /bin/sh 5 root 0:00 ps aux / # ip addr show 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 Now, you will see the Alpine Linux container with an isolated environment (PID and Network). That\u0026rsquo;s fully isolated from the Host machine.\nSingle-Host Container Networking from Scratch # TLDR;\n#!/usr/bin/env sh # # CONTAINER (A): Alpine Linux Container # ip link add veth0 type veth peer name veth1 ip link set veth1 netns \u0026#34;${ALPINE_CONTAINER_PID}\u0026#34; ip link set dev veth0 up ip addr add dev veth1 172.19.35.2/24 ip link set lo up ip link set veth1 up # # CONTAINER (B): Tiny Linux Container # ip link add veth2 type veth peer name veth3 ip link set veth3 netns \u0026#34;${TINY_CONTAINER_PID}\u0026#34; ip link set dev veth2 up ip addr add dev veth3 172.19.35.3/24 ip link set lo up ip link set veth3 up # # Create a bridge network and bring it up. # ip link add br0 type bridge ip link set veth0 master br0 ip link set veth2 master br0 ip addr add dev br0 172.19.35.1/24 ip link set br0 up In this section, we will configure Container networking from scratch, and learn how its networking works and how Containers communicate at the networking layer.\nOverview of Virtual Ethernet and Bridge Networking #Virtual Ethernet (VETH) and Bridge Networking are key components of Linux virtual networking that enable communication between Containers and the Host Linux system. They are widely used in Containerization technologies, such as Docker and Kubernetes.\nVirtual Ethernet (VETH) # Photo Credit: Virtual Ethernet (VETH) by Red Hat Developers\nVirtual Ethernet (VETH) is a pair of virtual network interfaces that act like a pipe: whatever is sent in one end is received by the other. They are commonly used to connect Network namespaces to the Host machine or other Network namespaces.\nA veth pair consists of two interfaces: one in the host machine\u0026rsquo;s Network namespace and one in the container\u0026rsquo;s Network namespace.\nPackets sent through one interface are received by the other.\nThis allows communication between the container and the host or other containers.\nBridge Networking # Photo Credit: Bridge Networking by Red Hat Developers\nBridge Network is a virtual network switch that connects multiple Network interfaces together. It allows Containers to communicate with each other.\nA bridge acts as a Layer 2 device, forwarding Ethernet frames between connected interfaces.\nContainers or virtual machines are connected to the bridge via VETH pairs.\nThe bridge can be connected to the host\u0026rsquo;s physical network interface to provide external connectivity.\nReference https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking\nContainer A (Alpine Linux): Setting up VETH Network #In this section, we will setup the VETH network on the Alpine Linux container.\nSame as previous, we will create a process, the Alpine Linux container, in isolated PID (Process ID), Mount, and Network namespaces using the command-line tools, unshare, chroot.\n$ cd ${HOME}/containers/alpine-linux $ sudo unshare --pid --mount --net \\ -f chroot . \\ env -i \\ HOME=/root \\ HOSTNAME=alpine-linux \\ /bin/sh $ mount -t proc proc /proc Then, get the container\u0026rsquo;s PID from your Host Linux machine. The Alpine Linux container\u0026rsquo;s PID is 25473.\nzawzaw@fedora-linux:~]$ ps -C sh PID TTY TIME CMD 25473 pts/6 00:00:00 sh Then, set the ALPINE_CONTAINER_PID environment variable with the export command. This PID is required to set when creating the VETH network.\n$ export ALPINE_CONTAINER_PID=25473 On the Host Linux machine, setup a veth network pair, veth0, veth1 with the ip command-line tool.\n[zawzaw@fedora-linux:~]$ sudo ip link add veth0 type veth peer name veth1 [zawzaw@fedora-linux:~]$ sudo ip link set veth1 netns \u0026#34;${ALPINE_CONTAINER_PID}\u0026#34; [zawzaw@fedora-linux:~]$ sudo ip link set dev veth0 up On the Alpine Linux container and set an IP address 172.19.35.3 to the veth1 network device and bring up.\n/ # ip addr add dev veth1 172.19.35.3/24 / # ip link set lo up / # ip link set veth1 up On the Host Linux machine, check the Network interfaces and IP addresses:\n[zawzaw@fedora-linux:~]$ ip addr show veth0 11: veth0@if10: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether c6:23:c1:27:62:a8 brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::c423:c1ff:fe27:62a8/64 scope link proto kernel_ll valid_lft forever preferred_lft forever On the Alpine Linux container, check the Network interfaces and IP addresses.\n/ # ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 10: veth1@if11: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue state UP qlen 1000 link/ether 1e:c9:8c:80:fd:9a brd ff:ff:ff:ff:ff:ff inet 172.19.35.3/24 scope global veth1 valid_lft forever preferred_lft forever inet6 fe80::1cc9:8cff:fe80:fd9a/64 scope link valid_lft forever preferred_lft forever Now, you will see the VETH interfaces, veth0, veth1 are up and set its IP address to 172.19.35.3.\nContainer B (Tiny Linux): Setting up VETH Network #For Container (B), we will use the Tiny Linux root filesystem image that I\u0026rsquo;ve compiled from the Linux kernel source code with busybox. Read more on Building a minimal Linux system from Scratch and Booting in QEMU Emulator.\n[zawzaw@fedora-linux:~/containers/tiny-linux]$ tree . ├── bin ├── dev ├── linuxrc -\u0026gt; bin/busybox ├── proc ├── sbin ├── sys └── usr Download: Tiny Linux Root Filesystem\nIn this section, we will setup the VETH network on the Tiny Linux container. Same as previous, we will create a process, Container (B) also known as the Tiny Linux container, in isolated PID (Process ID), Mount, and Network namespaces using the command-line tools, unshare, chroot.\n$ cd ${HOME}/cd containers/tiny-linux $ sudo unshare --pid --mount --net \\ -f chroot . \\ env -i \\ HOME=/root \\ HOSTNAME=tiny-linux \\ /bin/sh $ mount -t proc proc /proc Then, get the container\u0026rsquo;s PID from your Host Linux machine. The Tiny Linux container\u0026rsquo;s PID is 29999.\n[zawzaw@fedora-linux:~]$ ps -C sh PID TTY TIME CMD 25473 pts/6 00:00:00 sh 29999 pts/9 00:00:00 sh Then, set the TINY_CONTAINER_PID environment variable with the export command. This PID is required to set when creating the VETH network.\n$ export TINY_CONTAINER_PID=29999 On the Host Linux machine, setup a veth network pair, veth2, veth3 with the ip command-line tool.\n[zawzaw@fedora-linux:~]$ sudo ip link add veth2 type veth peer name veth3 [zawzaw@fedora-linux:~]$ sudo ip link set veth3 netns \u0026#34;${TINY_CONTAINER_PID}\u0026#34; [zawzaw@fedora-linux:~]$ sudo ip link set dev veth2 up On the Container (B), Tiny Linux container and set an IP address 172.19.35.2 to the veth3 network device and bring up.\n/ # ip addr add dev veth3 172.19.35.2/24 / # ip link set lo up / # ip link set veth3 up On the Host Linux machine, check the Network interfaces and IP addresses.\n13: veth2@if12: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 06:97:e6:1f:d4:b3 brd ff:ff:ff:ff:ff:ff link-netnsid 2 inet6 fe80::497:e6ff:fe1f:d4b3/64 scope link proto kernel_ll valid_lft forever preferred_lft forever On the Container (B), Tiny Linux container, check the Network interfaces and IP addresses.\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 12: veth3@if13: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue qlen 1000 link/ether 72:37:8b:a8:26:7e brd ff:ff:ff:ff:ff:ff inet 172.19.35.2/24 scope global veth3 valid_lft forever preferred_lft forever inet6 fe80::7037:8bff:fea8:267e/64 scope link valid_lft forever preferred_lft forever Now, you will see the VETH network interfaces, veth2, veth3 are up and set its IP address to 172.19.35.2.\nBut now, you will notice that Containers can\u0026rsquo;t communicate with each other. To confirm this, you can test by running the ping command.\n(A) Alpine Linux Container\u0026rsquo;s IP address: 172.19.35.3 (B) Tiny Linux Container\u0026rsquo;s IP address: 172.19.35.2 For example, ping 172.19.35.3 from the Tiny Linux container. It will not work and is not reachable network from one to another one because we need to setup a Bridge network.\n/ # ping -c 5 172.19.35.3 PING 172.19.35.3 (172.19.35.3): 56 data bytes --- 172.19.35.3 ping statistics --- 5 packets transmitted, 0 packets received, 100% packet loss In the next section, you\u0026rsquo;ll learn how to setup a Bridge network to forward the network packets between two containers.\nSetting Up Bridge Network #We now have two containers, Alpine Linux and Tiny Linux, running in fully isolated PID, Mount and Network Linux namesapces. They also have the Virtual Enthernet (VETH) pair in the same Network Linux namespace.\nIn this section, we will continue to setup a Bridge network to forward the network packets to the Two Containers.\nOn the Host Linux machine, setup a Bridge network and attach it to the veth0 and veth2 network interfaces.\n[zawzaw@fedora-linux:~]$ sudo ip link set veth0 master br0 [zawzaw@fedora-linux:~]$ sudo ip link set veth2 master br0 [zawzaw@fedora-linux:~]$ sudo ip addr add dev br0 172.19.35.1/24 Then, set an IP address 172.19.35.1 to the br0 network interface.\n[zawzaw@fedora-linux:~]$ sudo ip link set br0 up [zawzaw@fedora-linux:~]$ ip addr show br0 14: br0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 06:97:e6:1f:d4:b3 brd ff:ff:ff:ff:ff:ff inet 172.19.35.1/24 scope global br0 valid_lft forever preferred_lft forever inet6 fe80::497:e6ff:fe1f:d4b3/64 scope link proto kernel_ll valid_lft forever preferred_lft forever Then, you can test network reachability between Two Containers by running the ping command-line tool.\nFor example, ping 172.19.35.3 from the Tiny Linux container.\n/ # ping -c 5 172.19.35.3 PING 172.19.35.3 (172.19.35.3): 56 data bytes 64 bytes from 172.19.35.3: seq=0 ttl=64 time=0.070 ms 64 bytes from 172.19.35.3: seq=1 ttl=64 time=0.041 ms 64 bytes from 172.19.35.3: seq=2 ttl=64 time=0.059 ms 64 bytes from 172.19.35.3: seq=3 ttl=64 time=0.072 ms 64 bytes from 172.19.35.3: seq=4 ttl=64 time=0.050 ms --- 172.19.35.3 ping statistics --- 5 packets transmitted, 5 packets received, 0% packet loss For example, ping 172.19.35.2 from the Alpine Linux container.\n/ # ping -c 5 172.19.35.2 PING 172.19.35.2 (172.19.35.2): 56 data bytes 64 bytes from 172.19.35.2: seq=0 ttl=64 time=0.051 ms 64 bytes from 172.19.35.2: seq=1 ttl=64 time=0.105 ms 64 bytes from 172.19.35.2: seq=2 ttl=64 time=0.042 ms 64 bytes from 172.19.35.2: seq=3 ttl=64 time=0.053 ms 64 bytes from 172.19.35.2: seq=4 ttl=64 time=0.052 ms --- 172.19.35.2 ping statistics --- 5 packets transmitted, 5 packets received, 0% packet loss Now, we can confirm that Two Containers can communicate with each others and work properly.\nReference Links:\nhttps://blog.mbrt.dev/posts/container-network https://labs.iximiuz.com/tutorials/container-networking-from-scratch https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking ","date":"15 March 2025","permalink":"/blog/deep-single-host-container-networking/","section":"BLOG","summary":"How Containers (Alpine Linux and Tiny Linux Cotaniners) running on a Single Host communicate at the underlying layer","title":"Containers from Scratch: Deep Dive into Single-Host Container Networking"},{"content":"","date":null,"permalink":"/tags/network/","section":"TOPICS","summary":"","title":"Network"},{"content":"","date":null,"permalink":"/tags/android/","section":"TOPICS","summary":"","title":"Android"},{"content":"Android Devices အတွက် Kernel Source ကနေ Kernel တခု ဘယ်လို Build မလဲဆိုတဲ့ အကြောင်းအရာကို ဒီ How-To article မှာ အဓိက ပြောသွားမှာဖြစ်ပါတယ်။ Android OS က Linux Kernel ကို Based ထားပြီး Android ရဲ့ Kernel က Modified ထားတဲ့ Linux Kernel တခုပါ။ Android မှာသုံံံံံးထား Linux Kernel branch တွေက Long Term Support(LTS) branch တွေ ဖြစ်ပါတယ်။ https://www.kernel.org မှာ Long term branch တွေကြည့်နိုင်ပါတယ်။ ဥပမာ Nexus 5X, 6, 6P မှာဆိုရင် “linux-3.10-y” ဆိုတဲ့ branch ကို သုံးပါတယ် Google Pixel/Pixel XL မှာဆိုရင် “linux-3.18-y” ဆိုတဲ့ LTS branch တွေ သုံးကြပါတယ်။ Android OS က Linux Kernel ပေါ်မှာ အခြေခံပြီး တည်ဆောက်ထားတာ ဖြစ်ပြီး Kernel ဆိုတာ OS တခုရဲ့ အရေးကြီးတဲ့ အစိတ်အပိုင်းတခုပါ။ CPU, Memory, Disaply စတဲ့ Hardware အစိတ်အပိုင်းတွေ နဲ့ Software နဲ့ကြား ချိတ်ဆက်ပြီး အလုပ်လုပ်တဲ့ နေရာမှာ Kernel က အရေကြီးတဲ့ အပိုင်းမှာ ပါဝင်ပါတယ်။ Android OS Architecture ရဲ့ Linux Kernel အပိုင်းမှာ Display Driver, Camera Driver, USB Driver, Bluetooth Driver, Audio Driver, Power Management အစရှိသဖြင့်ပါဝင်ပါတယ်။ နမူနာပြောပြရရင် ကျွန်တော့််ရဲ့ Nexus 5X မှာ ပုံမှန် built-in ပါတဲ့ Stock Kernel မှာ Double Tap to Wake/Sleep / Disaply နဲ့ ပတ်သက်တဲ့ KCAL - Advanced Color Control / Audio driver နဲ့ ပတ်သက်တဲ့ Sound Control with High Performance Audio စသဖြင့် မပါဝင်ကြပါဘူး။ ကိုယ့်မှာ C Programming Skill ရှိရင် Kernel source တခု ကနေ အဲဒီ Kernel features တွေ ရေးပြီး ပြန် Recompile လုပ်နိုင်ပါတယ်။ ဒီနေရာမှာ Google ရဲ့ Nexus/Pixel လိုမျိုး Stock Pure Android ဖုန်းတွေ မဟုတ်တဲ့ တခြား Android OEMs တွေဖြစ်တဲ့ (Samsung, HTC, Sony and etc…) စတဲ့ Company တွေရဲ့ဖုန်းတွေမှာ တော်တော်များမှာ အဲဒီ Features အနည်းနဲ့အများ ပါဝင်ကြပါတယ်။ ဘာလု့ိ အဆင်သင့်ပါလဲဆိုတော့ သူတို့ရဲ့ Company က သက်ဆိုင်ရာ Android Engineer တွေက Source ကနေ Modified လုပ်ထားပြီးသားဖြစ်နေလု့ိပါပဲ။ အဲဒီ Features တွေ Device drivers - Audio, Display, Camera, USB and etc… / Memory / Power Management ပိုင်းတွေက Low-level ထိဆင်းပြီး C Programming နဲ့ရေးကြပါတယ်။ Custom Android Kernel တခု Build ရတဲ့အကြောင်းက Kernel source ယူပြီး Features တွေ ထပ်ပေါင်းထည့်ဖို့အတွက် ဖြစ်ပါတယ်။\nRequirements # GNU/Linux based Operating System(OS) Device\u0026rsquo;s Kernel Source Git: Version Control System GCC Toolchins (ARM/ARM64) Kernel Sources #Kernel source တွေက ဖုန်းအမျိုးအစာပေါ် မူတည်ပြီး download ရမယ့် site တွေက ကွဲပြားသွားပါလိမ့်မယ်၊ လိုအပ်တဲ့ Link တွေ အောက်မှပေးထားပါမယ်။\nGoogle Nexus/Pixel ( Qualcomm Chipset Only) : https://android.googlesource.com/kernel/msm Google Nexus (For all Chipsets) : https://android.googlesource.com/kernel/ Sony Xperia : https://developer.sonymobile.com/downloads/xperia-open-source-archives/ LG : http://opensource.lge.com/index Samsung : http://opensource.samsung.com/reception.do HTC : https://www.htcdev.com/devcenter/downloads Xiaomi : https://github.com/MiCode OnePlus : https://github.com/OnePlusOSS Motorola : https://github.com/MotorolaMobilityLLC နောက်တခုက အမျိုးမျိုးသော Android Device တွေရဲ့ Kernel source တွေ တနေရာတည်းမှာ ရနိုင်တဲ့ နေရကတော့ LineageOS ROM Community ကြီးပဲဖြစ်ပါတယ်။ (ဒါပေမယ့် တခုတော့ရှိတယ် အဲဒီ LineageOS Source ကနေ Build လိိုုက်တဲ့ Kernel တခုဟာ သူ့ရဲ့ ROM နဲ့ AOSP based ROM တွေမှာပဲ အလုပ်လုပ်ပါလိမ့်မယ်၊ ဥပမာ Xiaomi Device တွေ အနေနဲပြောရရင် သူ့ရဲ့ StockROM (MIUI) မှာ LineageOS source ကနေ build ထားတဲ့ Kernel ကို သုံးလို့ရမှာ မဟုတ်ပါဘူး အလုပ်လုပ်မှာ မဟုတ်ပါဘူး၊ ဖုန်းက LineageOS တင် ထားဖို့လိုပါယ်။ https://github.com/LineageOS Toolchains #Kernel Source ကနေ compile ဖို့အတွက်ဆိုရင် Toolchains တခုလိုအပ်ပါတယ်၊ Toolchains မှာ ကိုယ့်ဖုန် ရဲ့ CPU arch ပေါ် မူတည်ပြီ ARM နဲ့ ARM64 ဆိုပြီး ၂မျိုး ရှိပါတယ်။ လိုအပ်တဲ့ Link တွေ အောက်မှာ ပေးထားပါတယ်။\narm : https://android.googlesource.com/platform/prebuilts/gcc/linux-x86/arm/arm-eabi-4.8/ arm64 : https://android.googlesource.com/platform/prebuilts/gcc/linux-x86/aarch64/aarch64-linux-android-4.9/ Downloading the Kernel Sources #ဒီ TUT ကို ကျွန်တော်မှာရှိတဲ့ Nexus 5X နဲ့ ဥပမာပေးပြီး ပြောသွားမှာပါ။ ကျန်တဲ့ဖုန်းတွေ အတွက်ကလည်း သဘောတရာက တူတူပါပဲ၊ Kernel Source download တဲ့ နေရာပဲ ကွာသွားမှာပါ။\nအရင်ဆုံး Terminal ကိုဖွင့်ပြီး ဒီ command လေးရိုက်လိုက်ပါ။ (Dir တခုဆောက်ပါမယ်) mkdir KernelName cd KernelName Nexus 5X အတွက် Kernel source download ဖို့ အတွက် ဒီ command လေး ရိုက်လိုက်ပါ။ အရင်ဆုံး ကိုယ့် Computer ထဲမှာ git install ထားဖို့ လိုပါတယ်။ git clone -b android-msm-bullhead-3.10-oreo-r4 --depth=1 https://android.googlesource.com/kernel/msm ပြီးရင် Kernel compile ဖို့အတွက် Toolchains download ရပါမယ်။ (ဒီနေရာမှာ တခု သတိထားဖို့လိုပါတယ် ကိုယ်ရဲ့ဖုန်း CPU arch က arm64 ဆို arm64 toolchains ကို download ပါ၊ မဟုတ်ဘူး arm ဆိုရင် arm toolchains ကို download ပါ) git clone https://android.googlesource.com/platform/prebuilts/gcc/linux-x86/aarch64/aarch64-linux-android-4.9 အဲဒါတွေအကုန်ပြီးသွာပြီ ဆိုရင် Kernel build ဆို အဆင်သင့် ဖြစ်ပါပြီ။ Building the Kernel # အရင်ဆုံး Kernel source နဲ့ toochains ကို ပထမက ဆောက်ထားတဲ့ KernelName (PuerZ-Kernel-N5X) ဆိုတဲ့ Dir ထဲမှာ နှစ်ခုလုံး အဆင်သင့် ရှိနေရပါမယ်။ Toolchain Name ကို AOSP-Toolchains လို့ အမည်ပေးလိုက်ပြီး၊ Nexus 5X Kernel Source Name ကို bullhead လို့ အမည် ပေးလိုက်ပါမယ်။ (အဆင်ပြေသလို Rename လိုက်ပါ ပြဿ နာ မရှိပါဘူး၊ တခုပဲ Toolchains Location ပြန် ပေးတဲ့ နေရာမှာ အဲဒီ Name တွေအတိုင်း အတိအကျသိ ဖို့ လိုပါတယ်) e.g : Toolchains location /home/zawzaw/PureZ-Kernel-N5X/AOSP-Toolchains e.g : Kernel Source location /home/zawzaw/PureZ-Kernel-N5X/bullhead ပြီးရင် ကိုယ့်ဖုန်းအတွက် download ထားတဲ့ Kernel source Folder ထဲ ဝင်လိုက်ပါ။ Right Click ထောက်ပြီး Terminal လေးကို ဖွင့်လိုက်ပါ။ ပထမဦးဆုံး လုပ်ရမှာ Kernel source ကနေ compile ဖို့အတွက် export ဆိုတဲ့ command ကို သုံးပြီး toolchains ကို Set new environment variable သွားလုပ်ရပါမယ်။ (export - Set a New Environmetn Variable) Type this command (အဲဒီမှာ bin/နောက်ကကောင်ကို toochains prefix လို့ခေါ်ပါတယ် အခု Google က ပေးထားတဲ့ Toochain တွေ ရဲ့ prefix တွေကို ပြောပြပါမယ်၊ ARM အတွက်ဆိုရင် \u0026ldquo;arm-eabi-\u0026rdquo; ၊ ARM64 အတွက်ဆိုရင် \u0026ldquo;aarch64-linux-android-\u0026rdquo; ဖြစ်ပါတယ်) export CROSS_COMPILE=${HOME}/PureZ-Kernel-N5X/AOSP-Toolchains/bin/aarch64-linux-android- ကိုယ့်ဖုန်းရဲ့ CPU arch က arm လား arm64 လား သိထားဖို့ အရင်လိုပါတယ် အရင်ဆုံး ကိုယ့်ဖုန်းရဲ့ arch ကို ပြောပေးဖို့ လိုပါတယ်။ Nexus 5X က arm64 ဖြစ်တဲ့အတွက် ဒီ command လေး ဆက်ရိုက်လိုက်ပါ။ (တကယ်လို့ ကိုယ့်ဖုန်းက arm ဆိုရင် arm64 နေရာမှာ armလို့ ပြောင်း ရိုက်လိုက်ပါ။ export ARCH=arm64 \u0026amp;\u0026amp; export SUBARCH=arm64 နောက်တခုက Kernel source ထဲမှာ Compile ထား output file တွေ ရှိရင် ရှင်း ပေးဖို့ လိုပါတယ်။ make clean \u0026amp;\u0026amp; make mrproper နောက်ထက်တခု သိဖို့ကတော့ ကိုယ့် build မယ့် Kernel ရဲ့ build kernel configuration ပါ။ ARM device ဆိုရင် kernelsource/arch/arm/configs/ အောက်မှာ ရှိပါတယ်။ ARM64 device ဆိုရင် kernelsource/arch/arm64/configs/ အောက်မှာ ရှိပါတယ်။ Nexus 5X အတွက်ဆိုရင် bullhead/arch/arm64/configs/bullhead_defconfig (bullhead_defconfig ဆိုတာ Nexus 5X အတွက် build မယ့် kernel configuration အပိုင်းပါပဲ) ကိုယ့်ဖုန်းအတွက် kernel defconfig ကို သိချင်ရင် KernelSource/build.config file လေးကို ဖွင့်ကြည်နိုင်ပါတယ်။ အရင်ဆုံး Kernel compile မလုပ်ခင် build configuration လုပ်ပေးဖို့ လိုပါတယ်။ make bullhead_defconfig ပြီးရင် Kernel compile ပါတော့မယ်၊ compile ဖို့အတွက် အောက်က command လေးရိုက်လိုက်ပါ။ make -j$(nproc --all) Compilation process time က ကိုယ့် Computer ရဲ့ CPU core ပေါ်မူတည်ပြီးကြာနိုင်ပါတယ်။\nအဲဒါတွေပြီးသွားရင် Compiler ကနေ Compile လုပ်သွားပါလိမ့်မယ်။ Build လိုက်တဲ့ Kernel zImage တွေက ARM ဆိုရင် - kernelsource/arch/arm/boot/အောက်မှာ ထွက်ပါတယ်၊ ARM64 ဆိုရင် - kernelsource/arch/arm64/boot/အောက်မှာ ထွက်သွားလိမ့်မယ်။ အဲဒါ တွေ အောင်မြင်သွာပြီး ဆိုရင် ကိုယ်ဖုန်းအတွက် Kernel Install ဖု့ိ FlashableZip ဘယ်လိုလုပ်မလဲ ဆိုတာ ဆက်ရေးပါမယ်။\n","date":"9 March 2018","permalink":"/blog/build-android-kernel/","section":"BLOG","summary":"How to build kernel for your Android devices from the source code","title":"Building Kernel For Android Devices"},{"content":" Zaw Zaw I\u0026rsquo;m Zaw Zaw from Pyay, Myanmar and an SRE/Platform Engineer. Full name is Zaw Zaw Thein. I’m publishing articles that focus on Linux, containerization, Container networking, Kubernetes, and Cloud-native technologies, and also writing about simple living, personal development \u0026amp; growth, and experience on the blog.\nI\u0026rsquo;m a Former Recognized Developer (RD) at @XDA-Developers Forums. I’ve previously contributed to the Android operating system, Linux kernel, and Android Open Source Project (AOSP) based Android custom firmware projects on XDA Android Forums. I’ve loved working with Low-level software and systems programming.\nCurrently, I’m working on Site Reliability Engineering (SRE), Linux, CI/CD tools, Containerization, and Kubernetes. I’ve always had a keen interest in Computer engineering, Systems programming, Linux kernel, Cloud computing, Containerization, Cloud-native technologies and Kubernetes. And I also love to take photographs and am interested in photography.\nEDUCATION # Studied Computer Engineering at Pyay Technological University (P.T.U) Graduated from Basic Education High School (Sinmizwe) PROFESSIONAL SKILLS # Programming \u0026amp; Scripting: C/C++, Java, Bash Shell Leadership: Team Management, Mentorship, SRE Practices Operating Systems: Linux (Debian, Ubuntu, Fedora, RHEL) Cloud Platforms: AWS (EC2, EKS, S3) Version Control System: Git Continuous Integration (CI): GitLab CI, GitHub Actions Continuous Delivery (CD): Argo CD, Flux CD Networking: TCP/IP, DNS, Load Balancing, Linux Virtual Networking, Ingress NGINX, Istio Service Mesh Databases: MongoDB, MySQL, PostgreSQL Containerization \u0026amp; Orchestration: Docker, Podman, Kubernetes IaC \u0026amp; Configuration Management: Terraform (Basic), Ansible, Kustomize, Helm Monitoring and Observability: Prometheus, Grafana, Loki, Jaeger, ELK Stack (ElasticSearch, Logstash, Kibana) WORK EXPERIENCE #Frontiir # Lead Platform Engineer ‣ Apr 2024 - Apr 2025\nLed the Platform Engineering team. Led the building and migrating of GitOps ArgoCD pipelines for automating app deployments for both development and production environments on Kubernetes. Maintained self-managed internal K3s Kubernetes Clusters. Migrated legacy apps to Kubernetes and Cloud-native environments. Mentored Junior Platform engineers in GitLab CI, GitOps ArgoCD, and Kubernetes, focused on team building and researched new tools. Platform Engineer ‣ May 2023 - Mar 2024\nDesigned and created multi-node K3s Kubernetes Clusters. Integrated APISIX API Gateway and Keycloak for the microservices project on Kubernetes. Configured GitLab Runners on Kubernetes for internal OSS and BSS projects and maintained them. Developed Ansible Playbooks to bootstrap the Kubernetes Nodes. Configured and deployed Prometheus and Grafana for monitoring database servers and Kubernetes clusters. Senior Associate SRE Engineer ‣ Nov 2021 – Apr 2023\nContainerized OSS Software systems and built Helm charts to deploy them on Kubernetes. Configured GitOps ArgoCD Pipelines for automating OSS microservices app deployments on Kubernetes. Migrated and configured the Istio Service Mesh for the OSS microservices project. Built an operator for automating updating app container images on Kubernetes. Associate SRE Engineer ‣ Sep 2020 – Oct 2021\nMoved to the SRE team. Containerized apps and developed GitLab CI pipelines. Associate System Engineer ‣ Jan 2019 - Aug 2020\nWorked on building, porting and customizing the Linux kernel and embedded Android operating system (OS) for AMLogic SoC-based Android TV hardware devices. XDA-Developers # Recognized Developer ‣ 2017 - 2019\nContributed to Android, Linux kernel and Android open-source project, AOSP-based Android firmware projects as Recognized Developer and Contributor on XDA Community Forums. Senior Member ‣ 2016 - 2017\nContributed to some Android firmware and MOD projects as Senior Member on XDA Community Forums. INTERESTS # Computer Engineering Linux kernel Systems Programming Operating Systems SRE (Site Reliability Engineering) Cloud Computing Cloud-native Technologies Virtualization Containerization Photography Sharing, Reading and Writing ","date":null,"permalink":"/about/","section":"Welcome to ZawZaw.blog","summary":"","title":"ABOUT"},{"content":" Photos about Macro, City, Nature, and Landscape Photography I am also interested in photography and I love to take photographs as an Amateur Photographer. Most photos are about Macro, City, Nature, and Landscape photography, and are taken with the Google Nexus and Pixel devices.\nYou can see the photos on the following Pixieset Gallery website, Google Photos Album, and Instagram.\nPixieset: https://thezawzaw.pixieset.com\nGoogle Photos: https://photos.app.goo.gl/SJ9NYCk8so8oJRQz7\nInstagram: https://www.instagram.com/thezawzaw\n","date":null,"permalink":"/photography/","section":"Welcome to ZawZaw.blog","summary":"","title":"PHOTOGRAPHY"}]