[{"content":" LATEST POSTS \u0026amp; ARTICLES Subscribe to the Newsletter ","date":null,"permalink":"/blog/","section":"BLOG","summary":"","title":"BLOG"},{"content":"","date":null,"permalink":"/tags/habit/","section":"TOPICS","summary":"","title":"Habit"},{"content":"","date":null,"permalink":"/tags/personal-development/","section":"TOPICS","summary":"","title":"Personal-Development"},{"content":"Around the end of last year, I bought the Atomic Habits book at Fully Booked Iloilo, a bookstore in Iloilo City in the Philippines, and I\u0026rsquo;ve learned how habits work scientifically, how to build good habits in 4 simple steps, and how to break bad ones from the Atomic Habits book.\nIn this article, I will share about behavior change, how habits work, how to build good habits and break bad ones simply, and why small habits lead to long-term and remarkable results, as the Atomic Habits book summary.\nHabits are the compound interest of self-improvement. That means small and consistent habits lead to remarkable and long-term results over time.\nSummary: Key Points #What you\u0026rsquo;ll learn in this article:\nA system of Atomic Habits and how it works. Why small habits make a big difference and are so effective. Building Identity-based habits and systems over goals. The science of how habits work and how to improve them. The Four Laws of Behavior Change for creating good habits and breaking bad ones. Introduction to Atomic Habits #James Clear\u0026rsquo;s Atomic Habits book provides a comprehensive and practical framework for building good habits and breaking bad ones. It\u0026rsquo;s a highly practical and impactful guide for personal development.\nThe core theme of the Atomic Habits book is 1% better every day, and this book focused on the power of tiny changes. That means building good habits, focusing on small, incremental changes that accumulate over time, leading to big and remarkable results, instead of relying on motivation and willpower.\nThe author explains details about The Four Laws of Behavior Change based on scientific research, psychological insights, and actionable strategies for building good habits and breaking bad ones.\nThe Fundamentals: 3 Keys of Atomic Habits #(1) Building Identity-Based Habits #James Clear has discussed Identity-based habits in the Atomic Habits book as a key for building lasting habits. He said changing our habits is challenging for two reasons:\nWe try to change the wrong thing. We try to change our habits in the wrong way. In this section, to understand the first point, we will explore and learn what identity-based habits are.\nThere are Three Layers of Behavior Change. You can imagine them like the layers of an onion.\nThe first layer is changing your outcomes: This level is concerned with changing your results. For example, losing weight, publishing a book. Most of the goals you set are associated with this level.\nThe second layer is changing your processes: This level is concerned with changing your habits and systems. For example, decluttering your desk for better workflow, developing a meditation practice. Most of the habits you build are associated with this level.\nThe third layer is changing your identity: This level is concerned with changing your beliefs. For example, your self-image, your worldview. Most of the beliefs, assumptions, and biases you hold are associated with this level.\nIn simple words,\nOutcomes are about what you get. Processes are about what you do. Identity is about what you believe.\nPhoto: Outcome-Based Habits vs Identity-Based Habits from the Atomic Habits book Identity-based habits like this:\n(3) Identity ⟶ (2) Processes ⟶ (1) Outcomes\nThe author said many people begin the process of changing their habits by focusing on what they want to achieve, which leads to Outcome-based habits. The alternative is to build Identity-based habits; that is to start by focusing on who we wish to become.\nBasically, Identity-based habits are focused on what type of person you become. A key to building lasting habits is focusing on creating a new identity first. The author explains true behavior change is identity change. You might start a habit because of motivation. But the only reason you will stick with one is that it becomes part of your identity.\nYour behaviors are usually a reflection of your identity. What you do is an indication of the type of person you believe that you are, either consciously or unconsciously.\nFor example, you can think like this:\nThe goal is to become a reader; instead of thinking of reading a book. The goal is to become a writer; instead of thinking of writing a book. This goal is to become a runner; instead of thinking of running a marathon. How to Implement Identity-Based Habits?\nThe author provides a two-step process for changing your identity:\nDecide the type of person you want to be. Prove it to yourself with small wins. Step (1): Decide the type of person you want to be. #This is the first and important step in your changing your habit. You need to clearly define the identity you want to be. And ensure it\u0026rsquo;s an identity you genuinely desire.\nFor example,\nIf you want to read a book, your identity could be \u0026ldquo;I am a reader,\u0026rdquo; instead of saying, \u0026ldquo;I want to read a book.\u0026rdquo; If you want to write a book, your identity could be \u0026ldquo;I am a writer,\u0026rdquo; instead of saying, \u0026ldquo;I want to write a book.\u0026rdquo; If you want to lose weight, your identity could be \u0026ldquo;I am a healthy and active person,\u0026rdquo; instead of saying, \u0026ldquo;I want to lose weight.\u0026rdquo; Step (2): Prove it to yourself with small wins. #Once you have defined the person you want to be, start taking small and consistent actions that align with your identity. Every action you take is like casting \u0026ldquo;a vote\u0026rdquo; for the type of person you become. Those small actions show what type of person I am.\nFor example,\nIf you want to be a healthy person, start with something like \u0026ldquo;I will eat two servings of vegetables with dinner.\u0026rdquo; If you want to be a writer, start with something like \u0026ldquo;I will write one sentence every day\u0026rdquo; or \u0026ldquo;I will write for 5 minutes, three days a week.\u0026rdquo; (2) Building Systems over Goals #Most of the people, including me, have goals. But most have failed to achieve their goals. That\u0026rsquo;s why they don\u0026rsquo;t have the systems to achieve goals. James Clear discussed problems of goals:\nProblem #1: Winners and Losers have the same goals. Problem #2: Achieving a goal is only a momentary change. Problem #3: Goals restrict your happiness. Problem #4: Goals are at odds with long-term progress. The author also explains the difference between goals and systems.\nGoals are about the results you want to achieve. Systems are about the processes that lead to those results.\nIf you are a coach, your goal might be to win a championship. Your system is the way you recruit players, manage your assistant coaches, and conduct practice. If you are an entrepreneur, your goal might be to build a successful business. Your system is how you test product ideas, hire employees, and run marketing campaigns. So, are goals are completely useless? Of course not.\nGoals are good for setting a direction, but systems are best for making progress.\nA System of Atomic Habits #The core theme or overall system of the Atomic Habits book is 1% better every day. That means building good habits focusing on small, incremental changes that accumulate over time, leading to big and remarkable results, instead of relying on motivation and willpower.\nPhoto: 1% Better Every Day from the Atomic Habits book James Clear encourages building overall systems and environments to make good habits inevitable and bad habits harder, instead of relying on only setting goals.\nAn atomic habit refers to a small or tiny change, a marginal gain, 1% improvement. Just as atoms are building blocks of molecules, atomic habits are building blocks of big, long-term, and remarkable results.\n(3) Why Small Habits Make a Big Difference #Here is a brief summary of the benefits of small habits and how they can make a big difference. These are the things I use myself. You can also read about them in the Atomic Habits book.\nMost of the people think that these small habits are not very effective and that they don’t produce good results. But in reality, they really do work. Why are small habits so effective?\nEase of doing: Small habits are easy for everyone to do. You can easily incorporate them into your daily routine.\nConsistency: Small habits are more consistent. This should not be done randomly, but rather as a regular daily routine—you should specify what you will do after each task. Having a daily routine is scientifically good for your brain health. This is because the brain knows that it will do the tasks it needs to do at that time without having to think too much. It is more comfortable for the brain. Otherwise, the brain would be thinking about what to do. For example, after breakfast, you will write or read a book.\nCompound Effect: What is the Compound Effect? By doing a small daily habit, you will get big results over time. If you do 1% better each day and repeat it for a year, you will lead to good results. This is mainly discussed in the Atomic Habits book.\nMentally, it motivates you to do more: By doing these small daily habits, you will feel like you have done something, which will motivate you to do more the next day. Another thing is that it is not a big deal, but every time you do something, you release dopamine. This gives you a sense of satisfaction and happiness, which motivates you to keep doing it.\nThe Science of How Habits Work #A habit is a behavior that has been repeated enough times to become automatic. And any habit can be broken down into a habit loop that involves four steps: cue, craving, response, and reward.\nThe Atomic Habits book focuses heavily on the science of habit formation, particularly the concept of the habit loop.\nThe Four Stages of a Habit or Habit Loop #The process of building a habit can be divided into four simple steps: cue, craving, response, and reward. This four-step pattern is the backbone of every habit, and your brain runs in the same order each time. Let\u0026rsquo;s explore and try to understand what a habit is, how it works, and how to improve it.\nPhoto: The habit loop or The four stages of a habit from the Atomic Habits book. Cue — The Trigger: The first step of the habit loop. The cue triggers your brain to initiate a behavior. It\u0026rsquo;s a piece of information that predicts a reward. Cues can be anything from a specific location, an emotional state, and a time of day. The cue is the first indication that we’re close to a reward; it naturally leads to a craving.\nCraving — The Motivation: The second step of the habit loop. Craving is the motivational force behind every habit. Once a cue is initiated, it leads to craving. Every craving is linked to a desire to change your internal state.\nResponse — The Action: The response is the third step of the habit loop. The response is the actual habit or actual behavior you perform, which can take the form of a thought or an action. Whether you perform the response depends on how motivated you are and how easy or difficult the action is.\nReward — The Reinforcement: The reward is the final step of the habit loop. The reward is the end goal of every habit. It\u0026rsquo;s the satisfaction you get from performing the response. In simple words, the reward is the good feeling or benefit you get immediately after doing the habit.\nThe author simply explains the cue triggers a craving, which motivates a response, which provides a reward, which satisfies the craving, and ultimately becomes associated with the cue.\nTo understand the habit loop or four stages of a habit—cue, craving, response, and reward clearly, please see the following examples. These examples are also described in the Atomic Habits book.\nExample One,\nCue: Your phone buzzes with a new text message. Craving: You want to learn the contents of the message. Response: You grab your phone and read the text. Reward. You satisfy your craving to read the message. Grabbing your phone becomes associated with your phone buzzing. Example Two,\nCue: You are answering emails. Craving: You begin to feel stressed and overwhelmed by work. You want to feel in control. Response: You bite your nails. Reward. You satisfy your craving to reduce stress. Biting your nails becomes associated with answering email. Example Three,\nCue: You wake up. Craving: You want to feel alert. Response: You drink a cup of coffee. Reward. You satisfy your craving to feel alert. Drinking coffee becomes associated with waking up. Example Four,\nCue: You smell a doughnut shop as you walk down the street near your office. Craving: You begin to crave a doughnut. Response: You buy a doughnut and eat it. Reward. You satisfy your craving to eat a doughnut. Buying a doughnut becomes associated with walking down the street near your office. The Four Laws of Behavior Change #These above four steps form a neurological feedback loop—cue, craving, response, and reward—that ultimately allows you to create automatic habits. This cycle is known as the habit loop.\nJames Clear, the author of the Atomic Habits book, introduces Four Laws of Behavior Change based on this habit loop as a practical framework for building good habits and breaking bad ones.\nYou can use these four laws of behavior change to build a good habit:\nThe 1st Law (Cue) ▸ Make it obvious. The 2nd Law (Craving) ▸ Make it attractive. The 3rd Law (Response) ▸ Make it easy. The 4th Law (Reward) ▸ Make it satisfying. You can invert these four laws of behavior change to break a bad habit:\nInversion of the 1st Law (Cue) ▸ Make it invisible. Inversion of the 2nd Law (Craving) ▸ Make it unattractive. Inversion of the 3rd Law (Response) ▸ Make it difficult. Inversion of the 4th Law (Reward) ▸ Make it unsatisfying. How to Build a Good Habit and Break a Bad Habit #The following Atomic Habits summary cheat sheet compresses the key ideas and insights to build a good habit and break a bad habit from the Atomic Habits book.\nPhoto: Summary cheat sheet from the Atomic Habits book. Photo: Summary cheat sheet from the Atomic Habits book. As we discussed above, we can use the four laws of behavior change to build a good habit and break a bad habit. Let\u0026rsquo;s explore and learn details in the next sections.\nThe 1st Law (Cue): Make it Obvious # For Creating a Good Habit\nTo start a new good habit, you need to make its cue unmistakable. The 1st Law, Make it Obvious, focuses on making the triggers for your desired behaviors as noticeable as possible.\nThis involves the following strategies:\nThe Habits Scorecard Implementation Intentions Habit Stacking Design Your Environment The Habits Scorecard #The habits scorecard is a simple exercise you can use to become more aware of your behavior. Make a list of your daily habits. Once you have a full list, look at each habit. If it\u0026rsquo;s a good habit, write \u0026ldquo;+\u0026rdquo;. If it\u0026rsquo;s a bad habit, write \u0026ldquo;-\u0026rdquo;. If it\u0026rsquo;s a neutral habit, write \u0026ldquo;=\u0026rdquo;.\nFor example,\nWake up = Turn off alarm = Check my phone - Scroll social media - Go to the bathroom = Weigh myself + Take a shower + Brush my teeth + Floss my teeth + Put on deodorant + Get dressed = Make a cup of tea + This habits scorecard helps you to become aware of current cues and your behaviors. But it will depend on your situation and goals.\nFor example,\nFor someone who is trying to lose weight, eating a bagel with peanut butter every morning might beme a bad habit. For someone who is trying to bulk up and add muscle, the same behavior might be a good habit.\nImplementation Intentions #An implementation intention is a plan you make beforehand about when and where to act. Clearly define when and where you will perform a new good habit.\nFormat,\nI will [BEHAVIOR] at [TIME] in [LOCATION]. For example,\nExercise: I will exercise for one hour at 5:00 PM in my local gym. Studying: I will study Spanish for 20 minutes at 6:00 PM in my bedroom. Habit Stacking - A Simple Plan to Overhaul Your Habits #The best way to start a new good habit is Habit Stacking. What is it? It is easy to say that after a current habit, you start a new habit.\nFormat,\nAfter I do [CURRENT HABIT], I will do [NEW HABIT]. For example,\nReading: After watching the news every night, I will read for 30 minutes. Exercise: After I take off my work shoes, I will immediately change into my workout clothes. Habit Stacking works for me. Doing this small habit every day is more effective than doing it for two hours a week. But you need to be specific and consistent about what you are going to do after that. I encourage you to try this method.\nDesign Your Environment # Motivation Is Overrated; Environment Often Matters More\nEvery habit is initiated by a cue, so make the cues for good habits visible and accessible in your environment. Creating obvious visual cues can draw your attention toward a desired habit. It\u0026rsquo;s easier to build new good habits in a new environment because you don\u0026rsquo;t need to fight old cues.\nHere are a few ways to design your environment and make the cues for your habits more obvious.\nExamples,\nIf you want to practice guitar more frequently, place your guitar stand in the middle of the living room. If you wan to drink more water, fill up a few water bottles each morning and place them in common locations around the house. Inversion of the 1st Law (Cue): Make it Invisible # For Breaking a Bad Habit\nThe Inversion of the 1st Law, Make it Invisible, means reducing exposure and removing the cues of your bad habits from your environment.\nThis involves the following strategies:\nReduce Exposure Change Your Environment Reduce Exposure: Make the triggers for bad habits invisible to see or access.\nFor example,\nUnsubscribe from tempting email lists. Keep unhealthy snacks out of sight in a cupboard or don\u0026rsquo;t buy them. Turn off notifications on your phone Put your gaming console in a closet. Change Your Environment: If certain environments trigger bad habits, avoid them or modify them.\nFor example,\nIf you overspend at the mall, avoid going to the mall. If you always snack when watching TV, watch TV in a different room or keep all food out of that room. The 2nd Law (Craving): Make it Attractive # For Creating a Good Habit\nThe 2nd Law, Make it Attractive, focuses on increasing your desire or craving for a good habit, or making bad habits less appealing. We are more likely to pursue behaviors that are associated with positive feelings or immediate rewards.\nThis involves the following strategies:\nTemptation Bundling Join a Culture Where Your Desired Behavior is the Norm Temptation Bundling #Temptation bundling means pairing an action you WANT to do with an action you NEED to do. This makes the necessary habit more attractive.\nFor example,\nI will listen to my favorite songs (want) while I read a book (need). I will listen to my favorite podcast (want) while I clean the dishes (need). Join a Culture Where Your Desired Behavior is the Norm #We are heavily influenced by the people around us. If your desired habit is the norm in your social group, you\u0026rsquo;ll find it more attractive.\nFor example,\nIf you want to learn English, find an English study group. Join a running club, find a study group, and participate in a community garden. Inversion of the 2nd Law (Craving): Make it Unattractive # For Breaking a Bad Habit\nThe Inversion of the 2nd Law, Make it Unattractive, means associating your bad habits with negative consequences or feelings.\nThis involves the following strategies:\nHighlight the Benefits of Avoiding Your Bad Habits Highlight the Benefits of Avoiding Your Bad Habits: To make your bad habits seem unattractive, highlight the benefits of avoiding your bad habits.\nFor example,\nBy avoiding eating this junk food, I will feel energized, light, and focused for the rest of the day. By avoiding procrastination and starting now, I will feel calm, in control, and confident, knowing I\u0026rsquo;m making progress. The 3rd Law (Response): Make it Easy # For Creating a Good Habit\nThe 3rd Law, Make it Easy, focuses on reducing the friction and effort required to perform good habits and designing your environment to make the desired action as effortless as possible.\nThis involves the following strategies:\nReduce Friction Prime the Environment The Two-Minute Rule Reduce Friction #This means decreasing the number of steps between you and your good habits.\nFor example,\nPrepare your coffee maker the night before so all you have to do is press a button. Place the book you want to read directly on your pillow or nightstand. Open it to the page you left off on. Prime the Environment #This means preparing your environment to make future actions easier.\nFor example,\nLay out your workout clothes the night before. Organize your workspace before leaving for the day. The Two-Minute Rule #The Two-Minute Rule states when you start a new habit, it should take less than two minutes to do.\n\u0026ldquo;Run three miles\u0026rdquo; becomes \u0026ldquo;Tie my running shoes.\u0026rdquo; \u0026ldquo;Read before bed each night\u0026rdquo; becomes \u0026ldquo;Read on page.\u0026rdquo; Consider simplifying any new habit to a version that can be completed in under two minutes. The idea is to make your habits as easy as possible to start.\nThe author explains a new habit should not feel like a challenge. And the actions that follow can be challenging, but the first two minutes should be easy. You can usually figure out the gateway habit that leads to your desired outcome by mapping out your goals on a scale from Very easy to Very hard.\nFor example,\nVery easy: Put on your running shoes. Easy: Walk ten minutes. Moderate: Walk ten thousand steps. Hard: Run a 5K. Very Hard: Run a marathon. Inversion of the 3rd Law (Response): Make it Difficult # For Breaking a Bad Habit\nThe Inversion of the 3rd Law, Make it Difficult, means increasing the friction or effort to perform the undesired action.\nThis involves the following strategies:\nIncrease Friction Use Commitment Devices Increase Friction: This means adding steps or effort to make the bad habit harder to do.\nFor example,\nIf you want to spend less time on social media, delete the apps from your phone and only access them on your computer.\nUse Commitment Devices: A commitment device is a choice you make in the present that controls your actions in the future. It\u0026rsquo;s a way to lock in your future behavior, bind you to good habits, and restrict you from bad ones.##\nFor example,\nGive your phone to a friend during study time. When Victor Hugo shut his clothes away so he could focus on writing. The 4th Law (Reward): Make it Satisfying # For Creating a Good Habit\nThe 4th Law, Make it Satisfying, focuses on ensuring that the reward for your good habit is immediate and pleasureable, and this law emphasizes that if the experience is immediately satisfying, a behavior is more likely to be repeated.\nThis involves the following strategies:\nImmediate Reinforcement Use Habit Trackers Immediate Reinforcement #Immediate reinforcement means giving yourself an immediate reward when you complete a habit. Choose a small, immediate reward that you genuinely enjoy.\nFor example,\nAfter your workout, listen to your favorite podcast. After writing for 30 minutes, watch your favorite funny YouTube video. Use Habit Trackers #A habit tracker is a simple way to measure whether you did a habit, like marking \u0026ldquo;X\u0026rdquo; on your calendar. Habit trackers can make your habits satisfying by providing clear evidence of your progress.\nFor example,\nMark an \u0026ldquo;X\u0026rdquo; on your calendar each day after you complete a habit. Physically cross it off your TODO list after you have finished your tasks. Inversion of the 4th Law (Reward): Make it Unsatisfying # For Breaking a Bad Habit\nThe Inversion of the 4th Law, Make it Unsatisfying, focuses on making the consequences of your bad habits painful or unsatisfying. If it is painful or unsatisfying, we are less likely to repeat a bad habit.\nThis involves the following strategies:\nAccountability Partner Habit Contracts and Promises Accountability Partner: An accountability partner can create an immediate cost to inaction. Tell someone your goals and let them check in on your progress, and then knowing someone is watching makes failure more unsatisfying.\nFor example,\nMessage or tell a friend if you skip reading, and they can send you a reminder.\nHabit Contracts and Promises: A public contract can be used to add a social cost to any behavior; that\u0026rsquo;s a powerful way to add an immediate, unsatisfying cost to a bad habit.\nMake a public declaration of your intention to break a bad habit. The social cost of failing can be a powerful dissatisfier.\nFor example,\nStarting today, I\u0026rsquo;m committing to a digital detox. No social media scrolling after 8:00 PM every day.\n","date":"25 July 2025","permalink":"/blog/atomic-habits-book-summary/","section":"BLOG","summary":"","title":"Summary of the Atomic Habits Book: How to Build Better Habits"},{"content":"","date":null,"permalink":"/tags/","section":"TOPICS","summary":"","title":"TOPICS"},{"content":" Welcome to ZawZaw.blog. The blog publishes mostly practical guides, from fundamentals to deep dives into SRE, Linux Internals, Container technology \u0026amp; networking, and Kubernetes, as well as other general topics such as life journey and experience.\nSubscribe to the PlatformStack Newsletter to get the latest articles and guides directly to your inbox.\nSubscribe to the Newsletter ","date":null,"permalink":"/","section":"Welcome to ZawZaw.blog","summary":"","title":"Welcome to ZawZaw.blog"},{"content":"","date":null,"permalink":"/tags/containers/","section":"TOPICS","summary":"","title":"Containers"},{"content":"This article is Part II of the previously published article, Containers from Scratch: Deep Dive into Single-Host Container Networking. In Part I, you\u0026rsquo;ve learned how two containers communicate on the same single host, also known as Single-host container networking.\nIn this article, Part II, you\u0026rsquo;ll learn how Containers (Container A and Container B) running on two different hosts (VMs) communicate and interact with each other using VXLAN networking, also known as Multi-host container networking, and I\u0026rsquo;ll also demonstrate how Multi-host container networking works at the underlying layer with built-in Linux command-line tools.\nSummary: Objectives #What you\u0026rsquo;ll learn in this article:\nBasic Multi-Host Networking concepts\nBasic Concepts of Kubernetes CNI (Container Network Interface) plugins\nWhat\u0026rsquo;s VXLAN (Virtual eXtensible Local-Area Network) and how it works\nHow Containers running on Different Hosts communicate and interact using VXLAN networking\nPrerequisites #Before you begin, make sure you\u0026rsquo;ve installed the following tools:\nLinux-based Two VMs (or) Servers Basic Networking Concepts Familiar with Linux networking tools, such as ip and brctl Overview of Multi-host Container Networking #You\u0026rsquo;ve previously learned how two containers communicate on the same single host, only one host (or) VM, also known as Single-host container networking. For example, Docker single host.\nIn Multi-host container networking facilitated by overlay networks, containers running on the different hosts (or) VMs can communicate with each other in the same network. Sometimes, we refer to it as Multi-node (or) Cluster networking. For example, Docker Swarm mode, Kubernetes.\nTechnically, we can use two different networking methods for Multi-host container networking.\nVXLAN (Virtual eXtensible Local-Area Network) is a tunneling protocol or network virtualization technology that provides tunneling a virtual Layer 2 network (overlay network) over the Layer 3 network (underlay network).\nDirect Routing (also known as Native or Simple Routing) is the Linux kernel\u0026rsquo;s built-in capabilities for forwarding network packets between different networks, rather than using specific software or network protocols. That means it\u0026rsquo;s no encapsulation, no overlay network. It\u0026rsquo;s just simple IP routing.\nFor Example, the following Kubernetes CNI (Container Network Interface) plugins use different networking modes. But some Kubernetes CNIs provide both VXLAN (Encapsulation) and Direct Routing (or Native Routing) modes.\nFlannel: VXLAN is the default networking mode of Flannel.\nCalico: Calico\u0026rsquo;s default networking mode is BGP (Border Gateway Protocol) or Direct Routing, but you can also use the VXLAN networking mode.\nCilium: Cilium also provides both VXLAN and Direct Routing (or Native Routing) modes. But the default networking mode is VXLAN or tunnel mode, also known as encapsulation mode.\nKube-router: The default networking mode of Kube-router is BGP or Direct Routing (or Native Routing) as the main routing mechanism and so on.\nYou can see the CNI full list on https://github.com/containernetworking/cni?tab=readme-ov-file#3rd-party-plugins.\nIn this article, I will focus on VXLAN Networking to demonstrate Multi-host container networking from scratch.\nWhat\u0026rsquo;s VXLAN and How it Works # Photo Credit to: RedHat Developers (developers.redhat.com) VXLAN (Virtual eXtensible Local-Area Network) is a tunneling protocol or network virtualization technology that provides for creating a virtual Layer 2 network (overlay network) over the Layer 3 network (underlay network).\nAn Overlay Network is a virtual or logical network built on top of an existing physical network (also known as underlay network). It provides services, such as network virtualization, segmentation, and tunneling. For examples; VPNs, VXLAN.\nAn Underlay Network is the physical network infrastructure that provides the actual connectivity. For example; network routers, switches.\nVXLAN encapsulates the Layer 2 Ethernet frames into UDP packets. This enables Layer 2 network (the data link layer, e.g: switch) traffic to traverse a Layer 3 network (the network layer, e.g; router and IP address). It\u0026rsquo;s especially used in data centers, cloud environments, scalable overlay networks for VMs, and containers.\nUse Cases #There are example use cases of VXLAN:\nCloud Networking — Connecting VMs across hosts. Container Networking — Kubernetes CNI (Container Network Interface) plugins. VXLAN Components #There are key components of VXLAN:\nVXLAN Tunnel Endpoint (VTEP): This is the core component that performs the encapsulation and decapsulation of VXLAN packets. VTEPs can be physical network devices or virtual switches within hypervisors (e.g: VMware). Each VTEP has a unique IP address in the underlay network.\nVXLAN Network Identifier (VNI): This is a 24-bit identifier that uniquely identifies each virtual network segment within the VXLAN overlay.\nUnderlay Network: This is the physical network that VXLAN traffic traverses. It provides the routing infrastructure for the encapsulated VXLAN packets.\nOverlay Network: This is the virtual network created by VXLAN, running on top of the underlay physical network. It allows VMs (or Servers) to communicate.\nHow VXLAN Works #Basically, VXLAN (Virtual eXtensible Local-Area Network) works by encapsulating L2 Ethernet frames in UDP/IP with VTEPs handling the mapping between virtual overlay and physical underlay networks.\nSimple Usage:\n$ ip link add \u0026lt;vx0\u0026gt; type vxlan id 100 local \u0026lt;10.0.0.100\u0026gt; remote \u0026lt;10.0.0.200\u0026gt; dev eth0 dstport 4789 Setup (1): Frame Arrival # A VM or Server sends an Ethernet frame. For example, Host (A) ⟶ Host (B). The frame reaches the local VTEP (VXLAN Tunnel Endpoint). Setup (2): Encapsulation # The VTEP checks the VNI (VXLAN Network Identifier) and destination MAC address. It then encapsulates the frame inside a UDP/IP packet: Outer Source IP: Local VTEP IP 10.0.0.100 Outer Destination IP: Remote VTEP IP 10.0.0.200 VNI: 100 (For example, 100) UDP Port: 4789 (The default VXLAN UDP port is 4789) Setup (3): Underlay Forwarding # The encapsulated packet is sent over the physical (underlay) network. Setup (4): Decapsulation at Remote VTEP # The remote VTEP (10.0.0.200) receives the packet. Then, it checks the following: UDP Port: 4789 ⟶ identifies it as VXLAN. VNI: 100 ⟶ determines which virtual network it belongs to. The ethernet frame is delivered to the correct destination VM or server. Multi-Host Container Networking from Scratch #In this section, I will focus on configuring the network for Two Containers — Container A and Container B, running on Two Different VMs (Hosts) to communicate with each other. Make sure you have two Linux VMs or servers. In this article, I will use two AWS EC2 Instances to demonstrate how Multi-Host Container networking works using the VXLAN networking mode.\nOur project setup looks like this. Container A and Container B are running on Two different hosts.\nContainer A (10.0.0.100) on Host (1) — Debian Linux VM (172.31.89.40) Container B (10.0.0.200) on Host (2) — Amazon Linux VM (172.31.94.69) How it Works # Diagram on How Multi-Host Container Networking Works\nContainer (A) and Container (B) are running on two different hosts (Host 1 and Host 2).\nVETH (Virtual Ethernet) pair veth0,veth1 that connects the network between Host and Container in the same Linux network namespace. veth0 on the Host machine and veth1 on the container.\nVXLAN (Virtual eXtensible Local-Area Network) creates a tunnel that connects Host (1) — Debian Linux VM and Host (2) — Amazon Linux VM.\nThen, the Bridge network br0 is a network switch that forwards network packets between VXLAN and VETH network interfaces. Then, Container (A) and Container (B) can communicate with each other.\nIn the next section, you\u0026rsquo;ll learn how to set up and configure the network in more detail.\nOn Host (1) Debian Linux VM #Configuring Container Network #Firstly, I will set up and configure the network for Container A on Host (1) — Debian Linux. For running Containers, we will use the Alpine Linux root filesystem image.\nMake sure you familiar with how to run a Container from scratch with unshare, chroot tools and you\u0026rsquo;ve learned how to run it in the previous article, Part I — Containers from Scratch: Deep Dive into Single-Host Container Networking.\nCreate a project directory and download the Alpine Linux root filesystem image. This setup is same as the previous (Part I) article.\n$ mkdir -p containers/alpine-linux $ curl -LO https://dl-cdn.alpinelinux.org/alpine/v3.21/releases/x86_64/alpine-minirootfs-3.21.3-x86_64.tar.gz Extract the alpine-minirootfs-3.21.3-x86_64.tar.gz tar file and clean up.\n$ tar -xzvf alpine-minirootfs-3.21.3-x86_64.tar.gz $ rm alpine-minirootfs-3.20.2-x86_64.tar.gz Project structure looks like this:\n~/containers/alpine-linux ├── bin ├── dev ├── etc ├── home ├── lib ├── media ├── mnt ├── opt ├── proc ├── root ├── run ├── sbin ├── srv ├── sys ├── tmp ├── usr └── var I will create and run Container A in an isolated PID (Process ID), mount, and network namespaces using the command-line tools, unshare, chroot.\nThe following command creates a Container (Container A) that is fully PID, mount, and network isolated from the Host (OS) machine and then mounts the /proc virtual filesystem.\n$ cd ~/containers/alpine-linux $ sudo unshare --pid --mount --net \\ -f chroot ./ \\ env -i HOSTNAME=alpine-linux \\ /bin/sh -c \u0026#34;mount -t proc proc /proc; exec /bin/sh;\u0026#34; Then, open another terminal on the your Host machine and get the container process ID with the following command.\nThe following command gets the current running container\u0026rsquo;s PID and sets the environment variable and then I will use this ENV variable when configuring the container network.\n$ export CONTAINER_PID=$(ps -C sh -o pid= | tr -d \u0026#39; \u0026#39;) Then, create a VETH (Virtual Ethernet) network pair veth0, veth1 with the ip command-line tool.\nThe following command creates a VETH pair veth0, veth1 and sets veth1 to the network namespace with the current running container PID and then brings veth0 up. The VETH device is like a local Ethernet tunnel, and a VETH pair consists of two interfaces — one in the host machine\u0026rsquo;s network namespace and another one in the container\u0026rsquo;s network namespace. In the above example, veth0 is in the host machine\u0026rsquo;s Net namespace and veth1 is in the container\u0026rsquo;s Net namesapce.\nsudo ip link add veth0 type veth peer name veth1 sudo ip link set veth1 netns ${CONTAINER_PID} sudo ip link set dev veth0 up Then, set the IP address of Container A by running the following command without entering into the container\u0026rsquo;s shell.\nThe following command sets the IP address 10.0.0.100 to the veth1 network interface, also known as Container A\u0026rsquo;s IP address and brings lo, veth1 up. You can use nsenter to exec commands without entering the container\u0026rsquo;s shell.\nContainer A\u0026rsquo;s IP address ⟶ 10.0.0.100\nsudo nsenter --target ${CONTAINER_PID} \\ --mount \\ --net \\ --pid \\ chroot ${HOME}/containers/alpine-linux \\ /bin/sh -c \u0026#34;ip addr add dev veth1 10.0.0.100/24; ip link set lo up; ip link set veth1 up\u0026#34; Then, I will create a bridge network and attach veth0 to the bridge network with the following command.\nThe following command creates a Bridge network interface named br0, attaches veth0 to the br0 bridge interface, sets the IP address, and brings it up. Make sure you create and configure the bridge network because we need to communicate between the Container and the Host machine. A bridge network is like a network switch that forwards packets between network interfaces that are connected to it.\n$ sudo ip link add br0 type bridge $ sudo ip link set veth0 master br0 $ sudo ip addr add dev br0 10.0.0.1/24 $ sudo ip link set br0 up Configuring VXLAN Network Interface #In this section, I will set up and configure VXLAN to create a tunnel between Host (1) — Debian Linux VM and Host (2) — Amazon Linux VM, and then you can communicate between Container A and Container B.\nOn the Host (1) Debian Linux VM, create a VXLAN interface with the ip command-line tool.\nThe following command creates a VXLAN interface named vxlan0 and brings it up. It creates a tunnel and connects two VMs or servers. Host (1) — Debian Linux VM (172.31.89.40) is local and Host (2) — Amazon Linux VM (172.31.94.69) is remote. Make sure you set the same VNI (id) on both Host (1) and Host (2).\nLocal IP Address ⟶ 172.31.89.40 (Debian Linux VM) Remote IP Address ⟶ 172.31.94.69 (Amazon Linux VM) VXLAN Network Identifier (VNI) ⟶ 100 Destination Port⟶ 4789 (Default UDP Port) Network Interface (Device) ⟶ enX0 (Ethernet network device on the Debian Linux VM) sudo ip link add vxlan0 \\ type vxlan \\ id 100 \\ local 172.31.89.40 \\ remote 172.31.94.69 \\ dstport 4789 \\ dev enX0 \u0026amp;\u0026amp; \\ sudo ip link set vxlan0 up Then, attach the vxlan0 interface to the bridge network device with the following command.\nThe following command attaches the vxlan0 interface to the br0 bridge network device. Previously, we\u0026rsquo;ve created this bridge device and make sure you attach your VXLAN interface to the bridge network device. We need to forward packets or connect the VETH and VXLAN interfaces because it\u0026rsquo;s necessary to communicate between Container A (running on Host 1) and Container B (running on Host 2).\nsudo ip link set vxlan0 master br0 Then, you can check it with the brctl command-line tool. Make sure your veth0 and vxlan0 are attached to the bridge br0 device.\n$ brctl show bridge name bridge id STP enabled interfaces br0 8000.8aea1d11531b no veth0 vxlan0 On Host (2) Amazon Linux VM #Configuring Container Network #Same as the previous Host (1) setup, I will set up and configure the network for Container B on Host (2) — Amazon Linux. To create and run the container, we will use the Alpine Linux root filesystem image.\nCreate a project directory and download the Alpine Linux root filesystem image. This setup is same as the previous (Part I) article.\n$ mkdir -p containers/alpine-linux $ curl -LO https://dl-cdn.alpinelinux.org/alpine/v3.21/releases/x86_64/alpine-minirootfs-3.21.3-x86_64.tar.gz Extract the alpine-minirootfs-3.21.3-x86_64.tar.gz tar file and clean up.\n$ tar -xzvf alpine-minirootfs-3.21.3-x86_64.tar.gz $ rm alpine-minirootfs-3.20.2-x86_64.tar.gz Project structure looks like this:\n~/containers/alpine-linux ├── bin ├── dev ├── etc ├── home ├── lib ├── media ├── mnt ├── opt ├── proc ├── root ├── run ├── sbin ├── srv ├── sys ├── tmp ├── usr └── var I will create and run Container B in an isolated PID (Process ID), mount, and network namespaces using the command-line tools, unshare, chroot.\nThis command creates a Container (Container B) that is fully PID, mount, and network isolated from the Host (OS) machine and then mounts the /proc virtual filesystem.\n$ cd ~/containers/alpine-linux $ sudo unshare --pid --mount --net \\ -f chroot ./ \\ env -i HOSTNAME=alpine-linux \\ /bin/sh -c \u0026#34;mount -t proc proc /proc; exec /bin/sh;\u0026#34; Then, open another terminal on the your Host machine and get the container process ID with the following command.\nThis command gets the current running container\u0026rsquo;s PID and sets the environment variable and then I will use this ENV variable when configuring the container network.\n$ export CONTAINER_PID=$(ps -C sh -o pid= | tr -d \u0026#39; \u0026#39;) Then, create a VETH (Virtual Ethernet) network pair veth0, veth1 with the ip command-line tool.\nThis command creates a VETH pair veth0, veth1 and sets veth1 to the network namespace with the current running container PID and then brings veth0 up. The VETH device is like a local Ethernet tunnel, and a VETH pair consists of two interfaces — one in the host machine\u0026rsquo;s network namespace and another one in the container\u0026rsquo;s network namespace. In the above example, veth0 is in the host machine\u0026rsquo;s Net namespace and veth1 is in the container\u0026rsquo;s Net namesapce.\nsudo ip link add veth0 type veth peer name veth1 sudo ip link set veth1 netns ${CONTAINER_PID} sudo ip link set dev veth0 up Then, set the IP address of Container B by running the following command without entering into the container\u0026rsquo;s shell.\nThis command sets the IP address 10.0.0.200 to the veth1 network interface, also known as Container B\u0026rsquo;s IP address and brings lo, veth1 up. You can use nsenter to exec commands without entering the container\u0026rsquo;s shell.\nContainer B\u0026rsquo;s IP address ⟶ 10.0.0.200\nsudo nsenter --target ${CONTAINER_PID} \\ --mount \\ --net \\ --pid \\ chroot ${HOME}/containers/alpine-linux \\ /bin/sh -c \u0026#34;ip addr add dev veth1 10.0.0.200/24; ip link set lo up; ip link set veth1 up\u0026#34; Then, I will create a bridge network and attach veth0 to the bridge network with the following command.\nThis command creates a Bridge network interface named br0, attaches veth0 to the br0 bridge interface, sets the IP address, and brings it up. Make sure you create and configure the bridge network because we need to communicate between the Container and the Host machine. A bridge network is like a network switch that forwards packets between network interfaces that are connected to it.\n$ sudo ip link add br0 type bridge $ sudo ip link set veth0 master br0 $ sudo ip addr add dev br0 10.0.0.2/24 $ sudo ip link set br0 up Configuring VXLAN Network Interface #In this section, I will set up and configure VXLAN to create a tunnel between Host (1) — Debian Linux VM and Host (2) — Amazon Linux VM, and then you can communicate between Container A and Container B.\nSame as the previous Host (1) setup, on the Host (2) Amazon Linux VM, create a VXLAN interface with the ip command-line tool.\nThis command creates a VXLAN interface named vxlan0 and brings it up. It creates a tunnel and connects two VMs or servers. In this setup, Host (2) — Amazon Linux VM (172.31.94.69) is local and Host (1) — Debian Linux VM (172.31.89.40) is remote. Make sure you set the same VNI (id) on both Host (1) and Host (2).\nLocal IP Address ⟶ 172.31.94.69 (Host 2 Amazon Linux VM) Remote IP Address ⟶ 172.31.89.40 (Host 1 Debian Linux VM) VXLAN Network Identifier (VNI) ⟶ 100 Destination Port ⟶ 4789 (Default UDP Port) Network Interface (Device) ⟶ enX0 (Ethernet network device on the Host 2 Amazon Linux VM) sudo ip link add vxlan0 \\ type vxlan \\ id 100 \\ local 172.31.94.69 \\ remote 172.31.89.40 \\ dstport 4789 \\ dev enX0 \u0026amp;\u0026amp; \\ sudo ip link set vxlan0 up Then, attach the vxlan0 interface to the bridge network device with the following command.\nThis command attaches the vxlan0 interface to the br0 bridge network device. Previously, we\u0026rsquo;ve created this bridge device and make sure you attach your VXLAN interface to the bridge network device. We need to forward packets or connect the VETH and VXLAN interfaces because it\u0026rsquo;s necessary to communicate between Container A (running on Host 1) and Container B (running on Host 2).\nsudo ip link set vxlan0 master br0 Then, you can check it with the brctl command-line tool. Make sure your veth0 and vxlan0 are attached to the bridge br0 device.\n$ brctl show bridge name bridge id STP enabled interfaces br0 8000.8aea1d11531b no veth0 vxlan0 Testing Network Connectivity #Now, you\u0026rsquo;ve configured the container network and VXLAN tunnel, and you can ping the containers\u0026rsquo; IP addresses to confirm if it works.\nPing Container A (10.0.0.100) from the Host 2 (Amazon Linux VM).\nping 10.0.0.100 Ping Container B (10.0.0.200) from the Host 1 (Debian Linux VM).\nping 10.0.0.200 Reference Links:\nhttps://blog.mbrt.dev/posts/container-network https://labs.iximiuz.com/tutorials/container-networking-from-scratch https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking The PlatformStack NewsletterFrom Fundamentals to Deep Dives: SRE, Linux Internals, Container technology \u0026amp; networking, and Kubernetes. Subscribe to get the latest articles and guides directly to your inbox.\nSubscribeWe won't send you spam. Unsubscribe at any time.\nBuilt with Kit ","date":"25 May 2025","permalink":"/blog/deep-multi-host-container-networking/","section":"BLOG","summary":"","title":"Containers from Scratch: Deep Dive into Multi-Host Container Networking (Part II)"},{"content":"","date":null,"permalink":"/tags/networking/","section":"TOPICS","summary":"","title":"Networking"},{"content":"This article focused on a deep dive into Container networking; how to run Containers and configure Container Networking from scratch using the tools, such as Linux Namespaces, chroot, unshare and ip. This article also provides a hands-on practical guide on how to run and configure from scratch using these tools. You\u0026rsquo;ll mainly learn how Container Networking works at the underlying layer (or low level), and then you\u0026rsquo;ll clearly understand how Docker Container Networking works.\nSummary: Objectives #What you\u0026rsquo;ll learn in this article:\nBasic Concepts of Containerization.\nLinux Namespaces, the foundation of modern Container technologies.\nHow Linux Namespaces work and How Containers isolate from the Host OS.\nHow Virtual Ethernet and Bridge networking work.\nHow Containers and Container Networking work at the underlying layer, also known as low level.\nPrerequisities #Before we begin, make sure you are familiar with the following tools:\nLinux Host (e.g., Ubuntu, Fedora, etc..)\nchroot ─ a user-space tool to interact with the chroot(2) system call, to change the root filesystem of the calling process.\nunshare ─ a user-space tool to interact with Linux kernel namespaces by invoking the unshare(2) system call, to create a new process in a new namespace that isolates the process ID, mount, IPC, network, and so on.\nip ─ a command-line tool to configure network interfaces, routing, and tunnels.\nLinux namespaces ─ a feature of the Linux kernel that isolates and virtualizes system resources for a collection of processes.\nNOTE: If you have installed any Linux distribution, these tools are built-in tools and features.\nIntroduction to Containers # Docker Containers vs Virtual Machines by Docker\nNowadays, containers are a popular topic, and most companies are using containers to build, ship and run application workloads in both development and production environments.\nBasically, containers are a way to package and deploy applications in an isolated and portal environment and they provide a standardized way to bundle an application\u0026rsquo;s code, dependencies and configuration into a single unit that can be easily deployed on the server.\nContainers use OS-level virtualization to create an isolated environment for running applications. OS-level virtualization is a technology that allows multiple isolated Operating Systems to run on the same hardware, also known as containerization. Containers share the Host OS\u0026rsquo;s kernel and hardware resources, such as CPU and memory that make resource efficiency.\nBenefits of using Containers are:\nPortability: Containers can be moved easily between different environments, such as development, QA, staging and production.\nConsistency: Containers ensure that the application and its dependencies are packaged into a single unit that can be deployed easily.\nScalability: Containers can be scaled up or down easily on demand.\nEfficiency: Containers are lightweight and share the Host OS\u0026rsquo;s kernel and hardware resources that make more efficient to run applications.\nSetup Project and Root Filesystem #Firstly, we will build and run Containers from scratch using the chroot and unshare command-line tools to understand how Containers work.\nProject Structure looks like:\n${HOME}/containers ├── alpine-linux │ ├── bin │ ├── dev │ ├── etc │ ├── home │ ├── lib │ ├── media │ ├── mnt │ ├── opt │ ├── proc │ ├── root │ ├── run │ ├── sbin │ ├── srv │ ├── sys │ ├── tmp │ ├── usr │ └── var └── tiny-linux ├── bin ├── dev ├── proc ├── sbin ├── sys └── usr Create a project directory named, containers and then, we will put the Linux root filesystems (Alpine Linux, Tiny Linux) into it.\n$ mkdir -p containers/alpine-linux In this article, we will use the Alpine Linux Mini root filesystem. Go to the Alpine Linux official website https://alpinelinux.org/downloads and download the Alpine Linux Mini root filesystem.\nAlpine Linux has supported the Alpine Mini root filesystem that is for containers and minimal chroots. That supports multiple system architectures, such as aarch64, armv7. riscv64, x86, x86_64 and so on.\n(Or)\nDownload with the curl command line tool. For example, x86_64 architecture.\n$ curl -LO https://dl-cdn.alpinelinux.org/alpine/v3.21/releases/x86_64/alpine-minirootfs-3.21.3-x86_64.tar.gz Put the downloaded Alpine Linux Mini rootfs file under the containers/alpine-linux directory and then, extract the mini rootfs tar file.\n$ tar -xzvf alpine-minirootfs-3.21.3-x86_64.tar.gz Then, clean up the alpine-minirootfs-3.21.3-x86_64.tar.gz tar file.\n$ rm alpine-minirootfs-3.20.2-x86_64.tar.gz Running Containers from Scratch #Chroot: Basic Concept of Containers #/ (Host Root Filesystem) ├── bin ├── dev ├── etc ├── home │ └── zawzaw/ │ └── containers/ │ ├── alpine-linux/ │ │ ├── bin │ │ ├── dev │ │ ├── etc │ │ ├── home │ │ ├── lib │ │ ├── proc │ │ ├── sbin │ │ └── var │ └── tiny-linux/ │ ├── bin │ ├── dev │ ├── init.sh │ ├── linuxrc -\u0026gt; bin/busybox │ ├── proc │ ├── sbin │ ├── sys │ ├── sbin │ └── usr ├── proc ├── sbin ├── sys ├── usr └── var chroot (Change Root) is a Basic Concept of Containers. chroot is an operation that changes the apparent root directory for the current running process and its children on Unix and Unix-like operating systems. Historically, the chroot system call was introduced in Unix Seventh Edition (Version 7) in 1979.\nThe chroot user-space program (or cleint command-line tool) functionality relies on kernel support because it calls the system call chroot(2) handled by the kernel. This means that while you can execute chroot in user space, its effects depend on kernel-level enforcement.\nchroot is a user-space program (or) client command-line tool. It calls the chroot(2) system call, which is handled by the kernel. It is commonly used for sandboxing processes (or) creating minimal environments for recovery and testing. A process inside chroot still runs with the same privileges (it does not enhance security like containers do). Using the chroot User-space Tool #On Linux, we can use the chroot client command-line tool, to interact with the chroot(2) system call (a kernel API function call) to change the root directory for the current running process.\nGo to the already created project directory ${HOME}/containers/alpine-linux and run the chroot command.\n$ cd $HOME/containers/alpine-linux $ sudo chroot . /bin/sh / # ls -l total 0 drwxr-xr-x 1 1000 1000 858 Jul 22 14:34 bin drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 dev drwxr-xr-x 1 1000 1000 540 Jul 22 14:34 etc drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 home drwxr-xr-x 1 1000 1000 272 Jul 22 14:34 lib drwxr-xr-x 1 1000 1000 28 Jul 22 14:34 media drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 mnt drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 opt dr-xr-xr-x 1 1000 1000 0 Jul 22 14:34 proc drwx------ 1 1000 1000 24 Jul 30 04:26 root drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 run drwxr-xr-x 1 1000 1000 790 Jul 22 14:34 sbin drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 srv drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 sys drwxr-xr-x 1 1000 1000 0 Jul 22 14:34 tmp drwxr-xr-x 1 1000 1000 40 Jul 22 14:34 usr drwxr-xr-x 1 1000 1000 86 Jul 22 14:34 var / # cat /etc/os-release NAME=\u0026#34;Alpine Linux\u0026#34; ID=alpine VERSION_ID=3.21.3 PRETTY_NAME=\u0026#34;Alpine Linux v3.21\u0026#34; HOME_URL=\u0026#34;https://alpinelinux.org/\u0026#34; BUG_REPORT_URL=\u0026#34;https://gitlab.alpinelinux.org/alpine/aports/-/issues\u0026#34; Then, you need to mount the proc virtual filesystem inside the chroot isolated environment.\n$ mount -t proc proc /proc This mount command mounts the /proc Virtual Filesystem (VFS) to the /proc directory inside the chroot (or) container environment.\nmount: This command to attach a filesystem to the directory tree. -t proc: Specifies the filesystem type as the proc virtual filesystem. proc: The source since proc is virtual, no physical device is used. /proc: The mount point where the filesystem will be attached in the chroot isolated root filesystem. Why do we need to mount the /proc filesystem?\nMounting the /proc filesystem inside the chroot environment is necessary because the /proc virtual filesystem is a critical component (or) feature that is information about the system and process provided by the Linux kernel to the user-space apps and tools. Without it, many user-space apps and tools (e.g; ps, top) that rely on /proc will not work properly.\nFor example, the ps tool will not work properly.\n$ ps aux Output:\n# Error: Could not read /proc/stat Read more details about the /proc virtual filesystem in the next section.\nThe /proc Virtual Filesystem #The proc filesystem (often referred to as /proc) is a Virtual Filesystem (VFS) also known as a Pseudo (or) Special Filesystem on Linux that provides a way to expose system and process information to users and user-space applications in a structured, file-like format by the Linux kernel.\nThat does not rely on physical storage devices, such as HDDs and SSDs. The files and directories in /proc are not stored on disk and exist only in memory, and are generated on-the-fly (or) exposed dynamically by the Linux kernel when the system is booted.\nKey Features of the /proc Filesystem:\n(1) Virtual and Dynamic:\nThe files and directories in /proc are generated dynamically by the Linux kernel. The files and directories in /proc don\u0026rsquo;t exist on disk; they are created in memory when read. (2) System and Process Information:\n/proc provides the detailed information about: Running processes (e.g; /proc/[PID] for each process). System hardware (e.g; CPU: /proc/cpuinfo, Memory: cat /proc/meminfo) Kernel configuration and runtime parameters. (3) Readable and Writable:\nMost files in /proc are readable (e.g; you can cat /proc/cpuinfo). Some files are writable, allowing you to modify kernel parameters at runtime (e.g; /proc/sys). After you run the mount -t proc proc /proc command inside the container, you can test the following commands.\ncat /etc/os-release: To check the running container\u0026rsquo;s Linux distribution.\ncat /proc/version: To check the Linux kernel version that is shared from the Host OS.\ncat /proc/cpuinfo: To check the CPU information that is shared from the Host OS.\ncat /proc/meminfo: To check the Memory information that is shared from the Host OS.\nps aux: To check all processes that are running inside the container.\nip addr show: To see all IP addresses inside the currently running container.\nFor example,\n/ # cat /etc/os-release NAME=\u0026#34;Alpine Linux\u0026#34; ID=alpine VERSION_ID=3.21.3 PRETTY_NAME=\u0026#34;Alpine Linux v3.21\u0026#34; HOME_URL=\u0026#34;https://alpinelinux.org/\u0026#34; BUG_REPORT_URL=\u0026#34;https://gitlab.alpinelinux.org/alpine/aports/-/issues\u0026#34; / # cat /proc/version Linux version 6.13.5-200.fc41.x86_64 (mockbuild@be03da54f8364b379359fe70f52a8f23) (gcc (GCC) 14.2.1 20250110 (Red Hat 14.2.1-7), GNU ld version 2.43.1-5.fc41) #1 SMP PREEMPT_DYNAMIC Thu Feb 27 15:07:31 UTC 2025 / # cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 142 model name : Intel(R) Core(TM) i7-8565U CPU @ 1.80GHz stepping : 12 microcode : 0xfc cpu MHz : 2900.231 cache size : 8192 KB ... / # cat /proc/meminfo MemTotal: 16210104 kB MemFree: 3241444 kB MemAvailable: 9901900 kB Buffers: 5496 kB Cached: 7888196 kB ... Then, you will notice that the Alpine Linux container is using the Host OS kernel, Fedora Linux, with the kernel version 6.13.5-200.fc41.x86_64 that is shared from the Host OS. And also shared CPU and Memory from the Host machine.\nThen, we wil test the currently running processes and IP addresses inside the container like this.\n/ # ps aux PID USER TIME COMMAND 1 root 0:05 /usr/lib/systemd/systemd --switched-root --system --deserialize=51 rhgb 2 root 0:00 [kthreadd] 3 root 0:00 [pool_workqueue_] 4 root 0:00 [kworker/R-rcu_g] 5 root 0:00 [kworker/R-sync_] 6 root 0:00 [kworker/R-slub_] 7 root 0:00 [kworker/R-netns] 9 root 0:00 [kworker/0:0H-ev] 12 root 0:00 [kworker/R-mm_pe] 14 root 0:00 [rcu_tasks_kthre] 15 root 0:00 [rcu_tasks_rude_] 16 root 0:00 [rcu_tasks_trace] 17 root 0:16 [ksoftirqd/0] 18 root 0:10 [rcu_preempt] 19 root 0:00 [rcu_exp_par_gp_] 20 root 0:00 [rcu_exp_gp_kthr] ... / # ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host noprefixroute valid_lft forever preferred_lft forever 2: wlo1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP qlen 1000 link/ether fa:8b:5b:09:59:51 brd ff:ff:ff:ff:ff:ff inet 192.168.55.127/24 brd 192.168.55.255 scope global dynamic noprefixroute wlo1 valid_lft 75600sec preferred_lft 75600sec inet6 fe80::fa24:a316:4e60:c881/64 scope link noprefixroute valid_lft forever preferred_lft forever ... In that time, you will notice that we will see all processes and network interfaces of the Host OS, Fedora Linux, from the chroot Alpine Linux container. It means that we have not isolated the process ID (PID) and network.\nIn the next section, we will learn how Linux namespaces work and how to use the unshare user-space client tool to interact with Linux namespaces to start a process in a namespace that isolates the process ID (PID), mount, IPC, network, and so on.\nLinux Namespaces #Namespaces are a feature of the Linux kernel that isolates and virtualizes system resources for a collection of processes. Namespaces have been released in the Linux kernel version 2.4.19 since 2002.\nNamespaces are the foundation of modern Containerization technologies, such as Docker and Podman. Namespaces enable multiple processes to have different views of the system, such as different process IDs, network interfaces, filesystems, and so on.\nDocumentation: https://man7.org/linux/man-pages/man7/namespaces.7.html\nTypes of Linux Namespaces:\nNamespace Manual Page Isolates PID (Process ID) pid_namespaces Isolates process IDs. Mount mount_namespaces Isolates the set of mounted filesystems. UTS (UNIX Timesharing System) uts_namespaces Isolates hostname and DNS name. IPC (Inter-process Communication) ipc_namespaces Isolates IPC resources, such as message queue and shared memory. Network network_namespaces Isolates network interfaces, IP addresses, routing tables, and port numbers. User user_namespaces Isolates user and group IDs. CGroup cgroup_namespaces Isolates the view of Control Groups (CGroups). How Linux Namespaces Work #The Namespaces API supported the following system calls and Namespaces are created using these system calls.\nclone() Creates a new process in a new namespace.\nunshare() Creates (or) moves the calling process to a new namespace.\nsetns() Allows a process to join an existing namespace.\nExample Use Cases\nConsider a Container running on a Linux Host machine:\nThe container has its own Mount namespaces, So it can have its own root filesystem that is isolated from the Host OS.\nThe container has its own UTS namespaces, So it can have its own hostname that is isolated from the Host OS.\nThe container has its own PID namespaces, So processes inside the container have its own PIDs that are isolated from the Host OS.\nThe container has its own Network namespaces, So it can have its own IP addresses and network interfaces that are isolated from the Host OS.\nUsing the unshare User-space Tool #The unshare command-line tool is a user-space tool to interact with Linux kernel namespaces by invoking the unshare(2) system call, to create a new process in a new namespace (or) move a process into an existing namespace that isolates the process IDs, mount points, IPC, network interfaces, and so on.\nThe unshare CLI client tool creates new namespaces for the calling process and it can create one or more of the following namespaces.\nPID (Process ID) Mount UTS (Hostname and domain name) IPC (Inter-process Communication) Network User Cgroup Command Options:\n--mount: Create a new mount namespace.\n--uts: Create a new UTS namespace (isolates hostname and domain name).\n--ipc: Create a new IPC namespace.\n--net: Create a new network namespace.\n--pid: Create a new PID namespace.\n--user: Create a new user namespace.\n--cgroup: Create a new cgroup namespace.\n--fork: Fork a new process to run the command (required for PID namespaces).\nGo to the project directory,\n$ cd containers/alpine-linux Then, we will create a process, the Alpine Linux container, in PID (Process ID), Mount and Network namespaces using the unshare and chroot CLI tools.\n$ sudo unshare --pid --mount --net -f chroot ./ /bin/sh / # ls -l total 4 drwxr-xr-x 1 1000 1000 858 Feb 13 23:04 bin drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 dev drwxr-xr-x 1 1000 1000 540 Feb 13 23:04 etc drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 home drwxr-xr-x 1 1000 1000 146 Feb 13 23:04 lib drwxr-xr-x 1 1000 1000 28 Feb 13 23:04 media drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 mnt drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 opt dr-xr-xr-x 1 1000 1000 0 Feb 13 23:04 proc drwx------ 1 1000 1000 24 Mar 12 09:10 root drwxr-xr-x 1 1000 1000 8 Feb 13 23:04 run drwxr-xr-x 1 1000 1000 790 Feb 13 23:04 sbin drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 srv drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 sys drwxr-xr-x 1 1000 1000 0 Feb 13 23:04 tmp drwxr-xr-x 1 1000 1000 40 Feb 13 23:04 usr drwxr-xr-x 1 1000 1000 86 Feb 13 23:04 var Then, mount the /proc virtual filesystem to see the system and process information.\n$ mount -t proc proc /proc Then, check running processes and network interfaces on the Alpine Linux container.\n/ # ps aux PID USER TIME COMMAND 1 root 0:00 /bin/sh 5 root 0:00 ps aux / # ip addr show 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 Now, you will see the Alpine Linux container with an isolated environment (PID and Network). That\u0026rsquo;s fully isolated from the Host machine.\nSingle-Host Container Networking from Scratch # TLDR;\n#!/usr/bin/env sh # # CONTAINER (A): Alpine Linux Container # ip link add veth0 type veth peer name veth1 ip link set veth1 netns \u0026#34;${ALPINE_CONTAINER_PID}\u0026#34; ip link set dev veth0 up ip addr add dev veth1 172.19.35.2/24 ip link set lo up ip link set veth1 up # # CONTAINER (B): Tiny Linux Container # ip link add veth2 type veth peer name veth3 ip link set veth3 netns \u0026#34;${TINY_CONTAINER_PID}\u0026#34; ip link set dev veth2 up ip addr add dev veth3 172.19.35.3/24 ip link set lo up ip link set veth3 up # # Create a bridge network and bring it up. # ip link add br0 type bridge ip link set veth0 master br0 ip link set veth2 master br0 ip addr add dev br0 172.19.35.1/24 ip link set br0 up In this section, we will configure Container networking from scratch, and learn how its networking works and how Containers communicate at the networking layer.\nOverview of Virtual Ethernet and Bridge Networking #Virtual Ethernet (VETH) and Bridge Networking are key components of Linux virtual networking that enable communication between Containers and the Host Linux system. They are widely used in Containerization technologies, such as Docker and Kubernetes.\nVirtual Ethernet (VETH) # Photo Credit: Virtual Ethernet (VETH) by Red Hat Developers\nVirtual Ethernet (VETH) is a pair of virtual network interfaces that act like a pipe: whatever is sent in one end is received by the other. They are commonly used to connect Network namespaces to the Host machine or other Network namespaces.\nA veth pair consists of two interfaces: one in the host machine\u0026rsquo;s Network namespace and one in the container\u0026rsquo;s Network namespace.\nPackets sent through one interface are received by the other.\nThis allows communication between the container and the host or other containers.\nBridge Networking # Photo Credit: Bridge Networking by Red Hat Developers\nBridge Network is a virtual network switch that connects multiple Network interfaces together. It allows Containers to communicate with each other.\nA bridge acts as a Layer 2 device, forwarding Ethernet frames between connected interfaces.\nContainers or virtual machines are connected to the bridge via VETH pairs.\nThe bridge can be connected to the host\u0026rsquo;s physical network interface to provide external connectivity.\nReference https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking\nContainer A (Alpine Linux): Setting up VETH Network #In this section, we will setup the VETH network on the Alpine Linux container.\nSame as previous, we will create a process, the Alpine Linux container, in isolated PID (Process ID), Mount, and Network namespaces using the command-line tools, unshare, chroot.\n$ cd ${HOME}/containers/alpine-linux $ sudo unshare --pid --mount --net \\ -f chroot . \\ env -i \\ HOME=/root \\ HOSTNAME=alpine-linux \\ /bin/sh $ mount -t proc proc /proc Then, get the container\u0026rsquo;s PID from your Host Linux machine. The Alpine Linux container\u0026rsquo;s PID is 25473.\nzawzaw@fedora-linux:~]$ ps -C sh PID TTY TIME CMD 25473 pts/6 00:00:00 sh Then, set the ALPINE_CONTAINER_PID environment variable with the export command. This PID is required to set when creating the VETH network.\n$ export ALPINE_CONTAINER_PID=25473 On the Host Linux machine, setup a veth network pair, veth0, veth1 with the ip command-line tool.\n[zawzaw@fedora-linux:~]$ sudo ip link add veth0 type veth peer name veth1 [zawzaw@fedora-linux:~]$ sudo ip link set veth1 netns \u0026#34;${ALPINE_CONTAINER_PID}\u0026#34; [zawzaw@fedora-linux:~]$ sudo ip link set dev veth0 up On the Alpine Linux container and set an IP address 172.19.35.3 to the veth1 network device and bring up.\n/ # ip addr add dev veth1 172.19.35.3/24 / # ip link set lo up / # ip link set veth1 up On the Host Linux machine, check the Network interfaces and IP addresses:\n[zawzaw@fedora-linux:~]$ ip addr show veth0 11: veth0@if10: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether c6:23:c1:27:62:a8 brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::c423:c1ff:fe27:62a8/64 scope link proto kernel_ll valid_lft forever preferred_lft forever On the Alpine Linux container, check the Network interfaces and IP addresses.\n/ # ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 10: veth1@if11: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue state UP qlen 1000 link/ether 1e:c9:8c:80:fd:9a brd ff:ff:ff:ff:ff:ff inet 172.19.35.3/24 scope global veth1 valid_lft forever preferred_lft forever inet6 fe80::1cc9:8cff:fe80:fd9a/64 scope link valid_lft forever preferred_lft forever Now, you will see the VETH interfaces, veth0, veth1 are up and set its IP address to 172.19.35.3.\nContainer B (Tiny Linux): Setting up VETH Network #For Container (B), we will use the Tiny Linux root filesystem image that I\u0026rsquo;ve compiled from the Linux kernel source code with busybox. Read more on Building a minimal Linux system from Scratch and Booting in QEMU Emulator.\n[zawzaw@fedora-linux:~/containers/tiny-linux]$ tree . ├── bin ├── dev ├── linuxrc -\u0026gt; bin/busybox ├── proc ├── sbin ├── sys └── usr Download: Tiny Linux Root Filesystem\nIn this section, we will setup the VETH network on the Tiny Linux container. Same as previous, we will create a process, Container (B) also known as the Tiny Linux container, in isolated PID (Process ID), Mount, and Network namespaces using the command-line tools, unshare, chroot.\n$ cd ${HOME}/cd containers/tiny-linux $ sudo unshare --pid --mount --net \\ -f chroot . \\ env -i \\ HOME=/root \\ HOSTNAME=tiny-linux \\ /bin/sh $ mount -t proc proc /proc Then, get the container\u0026rsquo;s PID from your Host Linux machine. The Tiny Linux container\u0026rsquo;s PID is 29999.\n[zawzaw@fedora-linux:~]$ ps -C sh PID TTY TIME CMD 25473 pts/6 00:00:00 sh 29999 pts/9 00:00:00 sh Then, set the TINY_CONTAINER_PID environment variable with the export command. This PID is required to set when creating the VETH network.\n$ export TINY_CONTAINER_PID=29999 On the Host Linux machine, setup a veth network pair, veth2, veth3 with the ip command-line tool.\n[zawzaw@fedora-linux:~]$ sudo ip link add veth2 type veth peer name veth3 [zawzaw@fedora-linux:~]$ sudo ip link set veth3 netns \u0026#34;${TINY_CONTAINER_PID}\u0026#34; [zawzaw@fedora-linux:~]$ sudo ip link set dev veth2 up On the Container (B), Tiny Linux container and set an IP address 172.19.35.2 to the veth3 network device and bring up.\n/ # ip addr add dev veth3 172.19.35.2/24 / # ip link set lo up / # ip link set veth3 up On the Host Linux machine, check the Network interfaces and IP addresses.\n13: veth2@if12: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 06:97:e6:1f:d4:b3 brd ff:ff:ff:ff:ff:ff link-netnsid 2 inet6 fe80::497:e6ff:fe1f:d4b3/64 scope link proto kernel_ll valid_lft forever preferred_lft forever On the Container (B), Tiny Linux container, check the Network interfaces and IP addresses.\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 12: veth3@if13: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue qlen 1000 link/ether 72:37:8b:a8:26:7e brd ff:ff:ff:ff:ff:ff inet 172.19.35.2/24 scope global veth3 valid_lft forever preferred_lft forever inet6 fe80::7037:8bff:fea8:267e/64 scope link valid_lft forever preferred_lft forever Now, you will see the VETH network interfaces, veth2, veth3 are up and set its IP address to 172.19.35.2.\nBut now, you will notice that Containers can\u0026rsquo;t communicate with each other. To confirm this, you can test by running the ping command.\n(A) Alpine Linux Container\u0026rsquo;s IP address: 172.19.35.3 (B) Tiny Linux Container\u0026rsquo;s IP address: 172.19.35.2 For example, ping 172.19.35.3 from the Tiny Linux container. It will not work and is not reachable network from one to another one because we need to setup a Bridge network.\n/ # ping -c 5 172.19.35.3 PING 172.19.35.3 (172.19.35.3): 56 data bytes --- 172.19.35.3 ping statistics --- 5 packets transmitted, 0 packets received, 100% packet loss In the next section, you\u0026rsquo;ll learn how to setup a Bridge network to forward the network packets between two containers.\nSetting Up Bridge Network #We now have two containers, Alpine Linux and Tiny Linux, running in fully isolated PID, Mount and Network Linux namesapces. They also have the Virtual Enthernet (VETH) pair in the same Network Linux namespace.\nIn this section, we will continue to setup a Bridge network to forward the network packets to the Two Containers.\nOn the Host Linux machine, setup a Bridge network and attach it to the veth0 and veth2 network interfaces.\n[zawzaw@fedora-linux:~]$ sudo ip link set veth0 master br0 [zawzaw@fedora-linux:~]$ sudo ip link set veth2 master br0 [zawzaw@fedora-linux:~]$ sudo ip addr add dev br0 172.19.35.1/24 Then, set an IP address 172.19.35.1 to the br0 network interface.\n[zawzaw@fedora-linux:~]$ sudo ip link set br0 up [zawzaw@fedora-linux:~]$ ip addr show br0 14: br0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 06:97:e6:1f:d4:b3 brd ff:ff:ff:ff:ff:ff inet 172.19.35.1/24 scope global br0 valid_lft forever preferred_lft forever inet6 fe80::497:e6ff:fe1f:d4b3/64 scope link proto kernel_ll valid_lft forever preferred_lft forever Then, you can test network reachability between Two Containers by running the ping command-line tool.\nFor example, ping 172.19.35.3 from the Tiny Linux container.\n/ # ping -c 5 172.19.35.3 PING 172.19.35.3 (172.19.35.3): 56 data bytes 64 bytes from 172.19.35.3: seq=0 ttl=64 time=0.070 ms 64 bytes from 172.19.35.3: seq=1 ttl=64 time=0.041 ms 64 bytes from 172.19.35.3: seq=2 ttl=64 time=0.059 ms 64 bytes from 172.19.35.3: seq=3 ttl=64 time=0.072 ms 64 bytes from 172.19.35.3: seq=4 ttl=64 time=0.050 ms --- 172.19.35.3 ping statistics --- 5 packets transmitted, 5 packets received, 0% packet loss For example, ping 172.19.35.2 from the Alpine Linux container.\n/ # ping -c 5 172.19.35.2 PING 172.19.35.2 (172.19.35.2): 56 data bytes 64 bytes from 172.19.35.2: seq=0 ttl=64 time=0.051 ms 64 bytes from 172.19.35.2: seq=1 ttl=64 time=0.105 ms 64 bytes from 172.19.35.2: seq=2 ttl=64 time=0.042 ms 64 bytes from 172.19.35.2: seq=3 ttl=64 time=0.053 ms 64 bytes from 172.19.35.2: seq=4 ttl=64 time=0.052 ms --- 172.19.35.2 ping statistics --- 5 packets transmitted, 5 packets received, 0% packet loss Now, we can confirm that Two Containers can communicate with each others and work properly.\nReference Links:\nhttps://blog.mbrt.dev/posts/container-network https://labs.iximiuz.com/tutorials/container-networking-from-scratch https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking The PlatformStack NewsletterFrom Fundamentals to Deep Dives: SRE, Linux Internals, Container technology \u0026amp; networking, and Kubernetes. Subscribe to get the latest articles and guides directly to your inbox.\nSubscribeWe won't send you spam. Unsubscribe at any time.\nBuilt with Kit ","date":"15 March 2025","permalink":"/blog/deep-single-host-container-networking/","section":"BLOG","summary":"","title":"Containers from Scratch: Deep Dive into Single-Host Container Networking"},{"content":"In Kubernetes, we typically need to create and use Persistent Volumes for stateful applications such as database servers, cache store servers, and so on. In this article, I will share how storage provisioning on Kubernetes works and how to deploy Persistent Volumes dynamically using storage provisioners on the Kubernetes cluster.\nIn this article. I will mainly focus on configuring the Local-Path and NFS storage provisioners and managing persistent storage on the Kubernetes On-premises cluster, also known as self-managed Kubernetes.\nSummary: Objectives #What you\u0026rsquo;ll learn in this article:\nBasic concepts of Kubernetes persistent storage.\nDifference between Static storage provisioning and Dynamic storage provisioning.\nHow storage provisioning on Kubernetes works.\nHow to setup Local-Path and NFS storage provisioners on Kubernetes.\nHow to create and provision Persistent Volumes on-demand or dynamically on Kubernetes.\nPrerequisites # Kubernetes Cluster\nkubectl, a client CLI tool to communicate with the cluster\nHelm package manager tool\nKubernetes Basics\nMake sure you are familiar with basic Kubernetes objects and resources. If you are a Kubernetes newcomer, you can learn Kubernetes Basics tutorial that provides a hands-on practical guide on the basics of Kubernetes, container orchestration system.\nBackground #Introduction to Kubernetes Persistent Volumes #Firstly, we need to understand basic concepts on Kubernetes Persistent Volumes and how Kubernetes creates and manages persistent volumes. So, we will learn the basics before we setup and configure storage provisioners on the Kubernetes platform.\nBasically, Kubernetes has the following main two API resources to manage persistent storage.\nPersistentVolume (PV) PersistentVolumeClaim (PVC) PersistentVolume (PV) represents a piece of storage in the Kubernetes cluster. PVs can be provisioned manually by a cluster administrator or dynamically provisioned using PersistentVolumeClaim (PVC) with a storage class, and PVs can be filesystems (physical disks) and cloud storage services such as Amazon EBS and Azure disk.\nPersistentVolumeClaim (PVC) represents a request for storage, such as the storage size, access mode, and storage class. Persistent Volumes can be provisioned dynamically using PVC and storage class with any storage provisioner.\nIt\u0026rsquo;s a simple introduction. I will explain more details on persistent volumes with examples and demonstrate how to use them in the next sections.\nStorage Provisioning on Kubernetes #Basically, Kubernetes has two ways of provisioning persistent volumes.\nStatic Dynamic Static Storage Provisioning #Static storage provisioning creates persistent volumes manually for apps that require data persistence. In this approach, a cluster administrator needs to create PVs manually on the Kubernetes cluster.\nFor example, create PV and PVC manually for data persistence of the MySQL database server.\nPersistentVolume:\napiVersion: v1 kind: PersistentVolume metadata: name: pv-mysql-example spec: storageClassName: manual # Set storageclass name to \u0026#34;manual\u0026#34; or empty. capacity: storage: 8Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain hostPath: path: \u0026#34;/data/mysql\u0026#34; PersistentVolumeClaim:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-mysql-example namespace: sandbox spec: storageClassName: manual # Set storageclass name to \u0026#34;manual\u0026#34; or empty. accessModes: - ReadWriteOnce resources: requests: storage: 8Gi MySQL Pod:\napiVersion: v1 kind: Pod metadata: name: mysql-example namespace: sandbox spec: containers: - name: mysql-example image: mysql:latest ports: - name: mysql-tcp protocol: TCP containerPort: 3306 volumeMounts: - mountPath: \u0026#34;/data/mysql\u0026#34; name: vol-mysql-data volumes: - name: vol-mysql-data persistentVolumeClaim: claimName: pvc-mysql-example ... You can use the default manual class name that does not require any storage provisioner. It can be used to create PersistentVolume, PVs manually. See tutorial, https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage\nKubernetes supports hostPath persistent volume for development and local testing. In this approach, you need to create PV and PVC manually, and create volume mounts using PVC pvc-mysql-example in the MySQL Pod. And then MySQL data will be stored in the /data/mysql path on the local Kubernetes node.\nDynamic Storage Provisioning #Dynamic storage provisioning enables and allows to create persistent volumes on-demand or dynamically on the Kubernetes cluster based on the StorageClass API object. Basically, StorageClass defines which storage provisioner should be used when creating persistent volumes on Kubernetes.\n📝 But, please note that you need to deploy the provisioner on the Kubernetes cluster and we will explore how to setup in the next section.\nIn this approach, you can use any StorageClass when you configure PersistentVolumeClaim (PVC), and then it will automatically create PersistentVolume (PV) on the Kubernetes cluster.\nFor example,\nPersistentVolumeClaim (PVC) with local-path storageclass:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-mysql-example namespace: sandbox spec: storageClassName: local-path accessModes: - ReadWriteOnce resources: requests: storage: 8Gi MySQL Pod:\napiVersion: v1 kind: Pod metadata: name: mysql-example namespace: sandbox spec: containers: - name: mysql-example image: mysql:latest ports: - name: mysql-tcp protocol: TCP containerPort: 3306 volumeMounts: - mountPath: \u0026#34;/data/mysql\u0026#34; name: vol-mysql-data volumes: - name: vol-mysql-data persistentVolumeClaim: claimName: pvc-mysql-example ... In the above example, it creates volume mounts using PVC pvc-mysql-example in the MySQL Pod. And then MySQL data will be stored in the path specified by the provisioner on the Kubernetes node.\nIt depends on the provisioner you deployed and the provisioner\u0026rsquo;s mount path configuration. For example, the default path of Rancher\u0026rsquo;s local-path provisioner on the Kubernetes node is /var/lib/rancher/k3s/storage/.\nIn the next section, we will learn how to setup the local-path and NFS provisioners on the Kubernetes cluster.\nSetting Up Local Path Provisioner #Local Path Provisioner provides the ability to create the local persistent storage on-demand or dynamically in each Kubernetes node. Basically, it uses hostPath or local to create and deploy local persistent volumes on the Kubernetes node automatically. It\u0026rsquo;s simpler to provision local persistent volumes.\nDocumentation is available at https://github.com/rancher/local-path-provisioner/blob/master/README.md\nInstallation #In this article, we will use Rancher\u0026rsquo;s local-path provisioner on a self-managed Kubernetes cluster.\nLocal Path Provisioner: https://github.com/rancher/local-path-provisioner\nInstall with kubectl,\n$ kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml The provisioner will be installed in the local-path-storage namespace by default. After installation, check the provisioner pod and storageclass.\n$ kubectl get pods --namespace local-path-storage NAME READY STATUS RESTARTS AGE local-path-provisioner-5cffd47f7-42nbw 1/1 Running 0 5d20h 📝 The StorageClass resource is a cluster-wide resource and has no namespace scope. You just need to run the kubectl get storageclass command.\n$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path rancher.io/local-path Delete WaitForFirstConsumer false 5d20h Using Local Path Provisioner #In this section, we will test creating a PersistentVolume for a Pod automatically using PVC with the local-path storageclass. I will demonstrate it using the Busybox container image.\nCreate a YAML named local-storage-busybox.yaml.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-local-example namespace: sandbox spec: accessModes: - ReadWriteOnce storageClassName: local-path resources: requests: storage: 8Gi --- apiVersion: v1 kind: Pod metadata: name: local-storage-example namespace: sandbox spec: containers: - name: busybox image: busybox:latest imagePullPolicy: IfNotPresent command: - sh - \u0026#39;-c\u0026#39; - \u0026gt;- while true; do echo \u0026#34;$(date) [$(hostname)] Hello from Local Persistent Volume.\u0026#34; \u0026gt;\u0026gt; /data/local/greet.txt sleep $((RANDOM % 5 + 300)) done volumeMounts: - name: vol-pvc-local mountPath: /data/local volumes: - name: vol-pvc-local persistentVolumeClaim: claimName: pvc-local-example Then, install with the kubectl command-line tool like this:\n$ kubectl apply -f local-storage-busybox.yaml How it Works # PersistentVolumeClaim (PVC) #In the PersistentVolumeClaim, configured with the local-path storageclass, access mode is set to ReadWriteOnce and 8Gi storage is requested. But, please NOTE that local or hostPath only supports ReadWriteOnce access mode.\nFor the detailed information about Access Modes, please see the Access Modes in NFS storage section.\nThen, it will be provisioned a PV (PersistentVolume) automatically by Local Path Provisioner via StorageClass because we\u0026rsquo;ve installed it and configured PVC (PersistentVolumeClaim) using the local-path storage class. So, it\u0026rsquo;s called dynamic storage provisioning and we only need to configure PVC (PersistentVolumeClaim) with storage class.\n📝 The PV resource is a cluster-wide resource and has no namespace scope. You just need to run the kubectl get pv command.\nCheck PV with the kubectl command-line tool like this,\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS VOLUMEATTRIBUTESCLASS REASON AGE pvc-7e22e4b8-09d8-4553-88fc-1aefeb7c1ac3 8Gi RWO Delete Bound sandbox/pvc-local-example local-path \u0026lt;unset\u0026gt; 54m Busybox Pod (Workload) #In the Pod, created a volume mount path, /data/local using the PersistentVolumeClaim (PVC) named pvc-local-example with the Busybox container image. Basically, Pod\u0026rsquo;s command or script creates a file named greet.txt, writes date and hostname data to this file every 5m.\nYou can check the data by executing into the Pod shell.\n$ kubectl exec -it local-storage-example --namespace sandbox -- sh $ cd /data/local/ $ cat greet.txt Output:\nSun Jan 19 05:31:11 UTC 2025 [local-storage-example] Hello from Local Persistent Volume. Sun Jan 19 05:36:14 UTC 2025 [local-storage-example] Hello from Local Persistent Volume. Sun Jan 19 05:41:16 UTC 2025 [local-storage-example] Hello from Local Persistent Volume. Sun Jan 19 05:46:16 UTC 2025 [local-storage-example] Hello from Local Persistent Volume. Sun Jan 19 05:51:18 UTC 2025 [local-storage-example] Hello from Local Persistent Volume. Worker Node #The actual data greet.txt file will be stored on the worker node\u0026rsquo;s local-path provisioner mount path and the mount path on the worker node depends on the provisioner. For example, Rancher\u0026rsquo;s local-path provisioner mount path is /var/lib/rancher/k3s/storage/ by default.\nThen, check if the data greet.txt file is stored on the worker node correctly or not.\nLog in to the worker node with SSH.\nThen, go to the /var/lib/rancher/k3s/storage/ path and check the data by running the following commands.\nroot@k8s-worker:/var/lib/rancher# cd /var/lib/rancher/k3s/storage root@k8s-worker:/var/lib/rancher/k3s/storage# ls -l pvc-7e22e4b8-09d8-4553-88fc-1aefeb7c1ac3_sandbox_pvc-local-example total 52 -rw-r--r-- 1 root root 45657 Jan 21 00:28 greet.txt $ cd pvc-7e22e4b8-09d8-4553-88fc-1aefeb7c1ac3_sandbox_pvc-local-example $ cat greet.txt Sun Jan 19 05:31:11 UTC 2025 [local-storage-example] Hello from Local Persistent Volume. Sun Jan 19 05:36:14 UTC 2025 [local-storage-example] Hello from Local Persistent Volume. Sun Jan 19 05:41:16 UTC 2025 [local-storage-example] Hello from Local Persistent Volume. Sun Jan 19 05:46:16 UTC 2025 [local-storage-example] Hello from Local Persistent Volume. Sun Jan 19 05:51:18 UTC 2025 [local-storage-example] Hello from Local Persistent Volume. The data directory name or persistent volume data on the worker node is provisioned in the following format.\n{pv_name}-{namespace}{pvc_name} pvc-7e22e4b8-09d8-4553-88fc-1aefeb7c1ac3_sandbox_pvc-local-example Setting up NFS Provisioner #NFS (Network File System) is a distributed file system protocol that allows you to store and mount data on the remote server, also known as the NFS server. That means a client user can access data over a network and server-client way to manage data storage.\nIn Kubernetes, we will use NFS Subdir External Provisioner for provisioning persistent volumes on the NFS server. Basically, NFS subdir external provisioner, is a dynamic provisioner that uses the already configured NFS server to provision and create Kubernetes persistent volumes automatically on its NFS server using the PVC (PersistentVolumeClaim) and storage class.\nDocumentation at available at https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/blob/master/README.md\nBefore you deploy the NFS provisioner on Kubernetes, make sure you install both the NFS server and client tool.\nSetup NFS Server #Install the NFS server package. It depends on your Linux distribution. In this article, we will install it on Ubuntu Linux.\nsudo apt update sudo apt install -y nfs-server Configure the NFS server /etc/exports configuration for access control list for filesystems that may be exported to NFS clients.\n📝 IPv4 address or hostname is your NFS clients IP addresses.\nFormat,\n/mount/path ip_addr_or_hostname(rw,sync,no_subtree_check,no_root_squash) For example,\n# /etc/exports: the access control list for filesystems which may be exported # to NFS clients. See exports(5). # # Example for NFSv2 and NFSv3: # /srv/homes hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check) # # Example for NFSv4: # /srv/nfs4 gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check) # /srv/nfs4/homes gss/krb5i(rw,sync,no_subtree_check) # /data/nfs 172.16.x.1(rw,sync,no_subtree_check,no_root_squash) /data/nfs 172.16.x.2(rw,sync,no_subtree_check,no_root_squash) /data/nfs 172.16.x.3(rw,sync,no_subtree_check,no_root_squash) Install NFS Client on Worker Nodes #Before you setup NFS storage provisioner, make sure you install the NFS client tool on the Kubernetes Worker nodes.\nOn Debian-based Linux systems,\nsudo apt install -y nfs-common On RHEL-based Linux systems, for example: Fedora Linux,\nsudo dnf install -y nfs-utils Install NFS Subdir External Provisioner #In this section, we will install the NFS subdir external provisioner with the Helm package manager.\nHelm Repo URL: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner\nAdd a Helm repository,\n$ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner Install the NFS subdir external provisioner with Helm like this,\nFormat:\nhelm install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --create-namespace \\ --namespace nfs-provisioner \\ --set nfs.server=ip_addr_or_hostname \\ --set nfs.path=/exported/path Example:\nhelm install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --create-namespace \\ --namespace nfs-provisioner \\ --set nfs.server=127.0.0.1 \\ --set nfs.path=/data/nfs nfs.server=127.0.0.1 Replace the IP address with your NFS IP address.\nnfs.path=/data/nfs Replace the mount path with your exported path or mount path on the NFS server.\nThe NFS subdir external provisioner will be installed in the nfs-provisioner namespace. After installation, check the NFS provisioner\u0026rsquo;s workload (Pods) and StorageClass with the kubectl command-line tool.\n$ kubectl get pods --namespace nfs-provisioner NAME READY STATUS RESTARTS AGE nfs-provisioner-infra-nfs-subdir-external-provisioner-665fhd2wn 1/1 Running 0 5d20h 📝 The StorageClass resource is a cluster-wide resource and has no namespace scope. You just need to run the kubectl get storageclass command.\n$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE nfs-client cluster.local/nfs-provisioner-nfs-subdir-external-provisioner Delete Immediate true 5d20h Using the NFS Subdir External Provisioner #In this section, we will test creating and provisioning a PersistentVolume (PV) for a Pod automatically using PersistentVolumeClaim (PVC) with the nfs-client storage class. I will demonstrate it using the Busybox container image.\nCreate a YAML file named nfs-storage-busybox.yaml that includes PersistentVolumeClaim (PVC) and Pod resources.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-nfs-example namespace: sandbox spec: accessModes: - ReadWriteMany storageClassName: nfs-client resources: requests: storage: 8Gi --- apiVersion: v1 kind: Pod metadata: name: nfs-storage-example namespace: sandbox spec: containers: - name: busybox image: busybox:latest imagePullPolicy: IfNotPresent command: - sh - -c - \u0026gt;- while true; do echo \u0026#34;$(date) [$(hostname)] Hello from NFS Persistent Volume.\u0026#34; \u0026gt;\u0026gt; /app/data/greet.txt sleep $((RANDOM % 5 + 300)) done volumeMounts: - name: vol-pvc-nfs mountPath: /app/data volumes: - name: vol-pvc-nfs persistentVolumeClaim: claimName: pvc-nfs-example Then, install with the kubectl command-line tool like this,\n$ kubectl apply -f nfs-storage-busybox.yaml How it Works # PersistentVolumeClaim (PVC) #In the PersistentVolumeClaim, configured with the nfs-client storage class, access mode is set to ReadWriteMany and 8Gi storage is requested.\nspec.accessModes Set accessModes to ReadWriteMany. Basically, an access mode defines how a PV (PersistentVolume) can be accessed by Pods. That specifies Who can mount the volume (one or multi Kubernetes nodes) and How the volume can be accessed (read-only or read-write).\nDocumentation is also available at https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\nThe available access modes are ReadWriteOnce, ReadOnlyMany, ReadWriteMany and ReadWriteOncePod. But it depends on storage providers, and we need to check which access mode is supported.\nReadWriteOnce (RWO): The persistent volume can be mounted as read-write by a single node at a time.\nReadOnlyMany (ROX): The persistent volume can be mounted as read-only by multi nodes simultaneously.\nReadWriteMany (RWX): The persistent volume can be mounted as read-write by multi nodes at the same time.\nReadWriteOncePod (RWOP) - Kubernetes v1.22+: The persistent volume can be mounted by only one Pod at a time.\nAccess Mode Multi Nodes Read-Write Common Storage Types ReadWriteOnce (RWO) ❌ No ✅ Yes e.g; Rancher\u0026rsquo;s local-path, AWS EBS ReadOnlyMany (ROX) ✅ Yes ❌ No e.g; NFS, CephFS ReadWriteMany (RWX) ✅ Yes ✅ Yes e.g; Azure Files, CephFS, NFS, Longhorn, OpenEBS and etc\u0026hellip; ReadWriteOncePod (RWOP) ❌ No (Only one Pod) ✅ Yes e.g; Block storage like RWO but single Pod restriction 📝 The ReadWriteOncePod access mode is only supported for CSI volumes and Kubernetes version 1.22 and up.\nspec.storageClassName Set storageClassName to nfs-client. It depends on what storage class you want to use.\nspec.resources.requests.storage Set storage size that PVC (PersistentVolumeClaim) requests storage from a PV (PersistentVolume).\nAfter deploying the above PVC (PersistentVolumeClaim) named pvc-nfs-example, it will be provisioned a PV (PersistentVolume) by the NFS subdir external provisioner using the nfs-client storage class.\nThen, check PV (PersistentVolume) resource with the kubectl command-line tool,\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS VOLUMEATTRIBUTESCLASS REASON AGE pvc-edf39de5-42e2-453a-a23f-c4f5f58cf69a 8Gi RWX Delete Bound sandbox/pvc-nfs-example nfs-client \u0026lt;unset\u0026gt; 6h46m Busybox Pod (Workload) #In the Pod, created a volume mount path, /app/data using the PersistentVolumeClaim (PVC) named pvc-nfs-example with the Busybox container image. Basically, Pod’s command or script creates a file named greet.txt, writes date and hostname data to this file every 5m.\nThen, you can check the data by executing into the Pod shell.\n$ kubectl exec -it nfs-storage-example --namespace sandbox -- sh $ cd /app/data $ cat greet.txt Output:\nTue Jan 28 23:10:38 UTC 2025 [nfs-storage-example] Hello from NFS Persistent Volume. Tue Jan 28 23:15:42 UTC 2025 [nfs-storage-example] Hello from NFS Persistent Volume. Tue Jan 28 23:20:44 UTC 2025 [nfs-storage-example] Hello from NFS Persistent Volume. Tue Jan 28 23:25:47 UTC 2025 [nfs-storage-example] Hello from NFS Persistent Volume. Tue Jan 28 23:30:48 UTC 2025 [nfs-storage-example] Hello from NFS Persistent Volume. NFS Server #The actual data greet.txt file will be stored on the NFS server\u0026rsquo;s mount path, /data/nfs/ and the mount path is specified when you set up the NFS server and the NFS subdir external provisioner.\nThen, check if the data greet.txt file is stored on the NFS server correctly or not.\nLog in to the NFS server with SSH.\nThen, go to the NFS mount path /data/nfs/ and check the data by running the ls and cat commands.\nzawzaw@nfs-dev-server:~$ cd /data/nfs/ zawzaw@nfs-dev-server:/data/nfs$ ls -l sandbox-pvc-nfs-example-pvc-edf39de5-42e2-453a-a23f-c4f5f58cf69a/ total 12 -rw-r--r-- 1 root root 10795 Jan 29 16:14 greet.txt $ cd sandbox-pvc-nfs-example-pvc-edf39de5-42e2-453a-a23f-c4f5f58cf69a $ cat greet.txt Tue Jan 28 23:10:38 UTC 2025 [nfs-storage-example] Hello from NFS Persistent Volume. Tue Jan 28 23:15:42 UTC 2025 [nfs-storage-example] Hello from NFS Persistent Volume. Tue Jan 28 23:20:44 UTC 2025 [nfs-storage-example] Hello from NFS Persistent Volume. Tue Jan 28 23:25:47 UTC 2025 [nfs-storage-example] Hello from NFS Persistent Volume. Tue Jan 28 23:30:48 UTC 2025 [nfs-storage-example] Hello from NFS Persistent Volume. The data directory or persistent volume data on the NFS server is provisioned as the following format.\n{namespace}-{pvc_name}-{pv_name} sandbox-pvc-nfs-example-pvc-edf39de5-42e2-453a-a23f-c4f5f58cf69a ","date":"31 January 2025","permalink":"/blog/guide-k8s-persistent-storage/","section":"BLOG","summary":"","title":"A Hands-on Practical Guide to K8s Persistent Storage"},{"content":"","date":null,"permalink":"/tags/kubernetes/","section":"TOPICS","summary":"","title":"Kubernetes"},{"content":"","date":null,"permalink":"/tags/storage/","section":"TOPICS","summary":"","title":"Storage"},{"content":"This article focuses on how to create a user and configure cluster roles and role bindings for that user using Kubernetes built-in RBAC authorization features. Basically, in Kubernetes, we can use the service account as a user, but it is like a non-human user.\nGenerally, every Kubernetes cluster has the default admin kubeconfig file. We usually use this admin kubeconfig file to log in to, access, and manage the Kubernetes cluster. We can generate a service account token and assign this service account or user to a specific cluster role, and then configure the kubeconfig file with its token and give access to the user.\nThen, a user or developer can log in to and access the Kubernetes cluster with the token or kubeconfig file using the Kubernetes Dashboard (or) the kubectl command-line tool.\nUse Case # Working with the team and want to give access to a user or developer with specific permissions on the Kubernetes cluster. For example, if you want to give access to a user with read-only access permissions.\nSummary: Objectives #What you\u0026rsquo;ll learn in this article:\nExplore built-in RBAC authorization features on Kubernetes.\nHow to configure a service account, cluster roles and role bindings.\nLearn a deep dive into RBAC verbs and how to grant permissions to a user.\nHow to generate a service account token for the user and use it in the kubeconfig file to access the Kubernetes cluster.\nBefore We Begin #Make sure you have installed the following tools.\nKubernetes Cluster The kubectl command-line tool (or) Kubernetes Dashboard Creating a User with RBAC Bindings #In this article, we will demonstrate how to create a service account, charlie with engineer role that has read-only permissions to access all resources on the Kubernetes cluster, except from Secrets resources.\nCreate a YAML file named k8s-user-charlie.yaml that includes and configures ServiceAccount, ClusterRole, and ClusterRoleBinding resources.\napiVersion: v1 kind: ServiceAccount metadata: name: charlie namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: engineer rules: - verbs: - get - list - watch apiGroups: - \u0026#39;\u0026#39; resources: - configmaps - endpoints - persistentvolumeclaims - pods - replicationcontrollers - replicationcontrollers/scale - services - nodes - persistentvolumeclaims - persistentvolumes - verbs: - get - list - watch apiGroups: - \u0026#39;\u0026#39; resources: - bindings - events - limitranges - namespaces/status - pods/log - pods/status - replicationcontrollers/status - resourcequotas - resourcequotas/status - verbs: - get - list - watch apiGroups: - \u0026#39;\u0026#39; resources: - namespaces - verbs: - get - list - watch apiGroups: - apps resources: - daemonsets - deployments - deployments/scale - replicasets - replicasets/scale - statefulsets - verbs: - get - list - watch apiGroups: - autoscaling resources: - horizontalpodautoscalers - verbs: - get - list - watch apiGroups: - batch resources: - cronjobs - jobs - verbs: - get - list - watch apiGroups: - extensions resources: - daemonsets - deployments - deployments/scale - ingresses - networkpolicies - replicasets - replicasets/scale - replicationcontrollers/scale - verbs: - get - list - watch apiGroups: - policy resources: - poddisruptionbudgets - verbs: - get - list - watch apiGroups: - networking.k8s.io resources: - networkpolicies - ingresses - verbs: - get - list - watch apiGroups: - storage.k8s.io resources: - storageclasses - volumeattachments --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: rolebinding-charlie roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: engineer subjects: - kind: ServiceAccount name: charlie namespace: kube-system After configuring RBAC bindings in the k8s-user-charlie.yaml YAML file, deploy it with the kubectl command-line tool on the Kubernetes cluster.\n$ kubect apply -f k8s-user-charlie.yaml Output:\nserviceaccount/charlie created clusterrole.rbac.authorization.k8s.io/engineer created clusterrolebinding.rbac.authorization.k8s.io/rolebinding-charlie created In the above RBAC configuration, the engineer ClusterRole specifies a set of permissions to access all resources of Kubernetes, except from Secrets, and then the ClusterRoleBinding assigned a ServiceAccount user charlie to the engineer role.\nLearn more about the detailed information on the RBAC configuration in the next section.\nUnderstanding the RBAC Configuration #ServiceAccount Creates a user for login and access the Kubernetes cluster.\nClusterRole Specifies rules that represent a set of permissions.\nverbs Specifies permissions to access Kubernetes resources. For example, get, list, watch, patch and etc..\nget - Read individual resources or objects, like viewing pod details.\nlist - List multiple resources of a certain type, like listing all pods in a namespace.\nwatch - Observe resources for real-time updates, often used by controllers to track changes. For example, kubectl get configmap \u0026ndash;watch.\ncreate - Create a new resource or object, like creating a new pod or deployment or service. For example, kubectl apply -f deployment.yaml.\ndelete - Remove individual resources, like deleting a specific pod.\nupdate - Update the entire object. An update operation replaces the entire resource specification. It overwrites the existing object with the new specification. For example, kubectl apply -f deployment.yaml.\npatch - Apply a partial update to a resource. A patch operation modifies specific fields selectively without affecting the rest of the object.\nFor example, kubectl patch deployment nginx-server -p '{\u0026quot;spec\u0026quot;: {\u0026quot;template\u0026quot;: {\u0026quot;spec\u0026quot;: {\u0026quot;containers\u0026quot;: [{\u0026quot;name\u0026quot;: \u0026quot;nginx\u0026quot;, \u0026quot;image\u0026quot;: \u0026quot;nginx:v2\u0026quot;}]}}}}')\napiGroups Specifies Kubernetes API groups. For example, Deployment is included in the apps API group.\nTo get Kubernetes API groups, run the kubectl api-resources command. Check the APIVERSION column and format is \u0026lt;api-group/version\u0026gt;.\n$ kubectl api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND configmaps cm v1 true ConfigMap namespaces ns v1 false Namespace pods po v1 true Pod secrets v1 true Secret services svc v1 true Service daemonsets ds apps/v1 true DaemonSet deployments deploy apps/v1 true Deployment replicasets rs apps/v1 true ReplicaSet statefulsets sts apps/v1 true StatefulSet resources Specifies which Kubernetes resources to access to the user. Same as above api groups, run the kubectl api-resources command and check the resource name in the NAME column. For example, deployment, services and so on.\nClusterRoleBinding Grants the permissions defined in the ClusterRole and assigned to the ServiceAccount user.\nroleRef.apiGroup\nSet the API group of the cluster role (rbac.authorization.k8s.io). roleRef.kind\nSet the resource name of the cluster role (ClusterRole). roleRef.name\nSet the name of the cluster role you configured. In this article, I created an engineer role. subjects.kind\nSet the resource name of service account. (ServiceAccount) subjects.name\nSet the ServiceAccount name you created. In this article, it\u0026rsquo;s charlie. subjects.namespace\nSet the namespace of your installed ServiceAccount resource. In this article, kube-system namespace. Reference: https://kubernetes.io/docs/reference/access-authn-authz/rbac\nGenerating a ServiceAccount Token #Previously, we have created a service account charlie user in the kube-system namespace. Next, we will create a service account token that we need to login and access the Kubernetes cluster and also need to use it in the kubeconfig file.\nGet the service account name you deployed previously.\n$ kubectl get serviceaccounts --namespace kube-system Output:\nNAME SECRETS AGE ... default 0 18d svclb 0 18d k8s-admin 0 18d kubernetes-dashboard-metrics-server 0 18d kubernetes-dashboard 0 18d charlie 0 21m Create a service account token for the charlie user.\n$ kubectl create token charlie --duration=8760h --output yaml --namespace kube-system Output:\napiVersion: authentication.k8s.io/v1 kind: TokenRequest metadata: creationTimestamp: \u0026#34;2024-11-25T04:13:16Z\u0026#34; name: charlie namespace: kube-system spec: audiences: - https://kubernetes.default.svc.cluster.local - k3s boundObjectRef: null expirationSeconds: 31536000 status: expirationTimestamp: \u0026#34;2025-11-25T04:13:16Z\u0026#34; token: eyJhbGciOiJSUzI1NiIsImtpZCI6Ii04blMtX1BpVHBQUUNxbXMxSEI3bkxRU1ZfNkVIR09UZGNiazlaWEoyaHMifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiLCJrM3MiXSwiZXhwIjoxNzY0MDQzOTk2LCJpYXQiOjE3MzI1MDc5OTYsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJjaGFybGllIiwidWlkIjoiMDAxOTYyNDUtNzhiNy00NzFkLThjODAtYWFhYWMyM2I2NjJjIn19LCJuYmYiOjE3MzI1MDc5OTYsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpjaGFybGllIn0.PokSrN-g4vz-9cl9d7_KuVl3UGh5q2Slww0G68uE4hrocSvrFizmCch4VkKCrzZ7anp9P3nWBvk8WVizo4nN5U_Wn3p-h7_nsvyTrJ7_FflNYS6w8fbfFVKl0vD31MVrpmj40DApkbIepnDKzKfvvNsf00zFoNyi2iB7iIgsrkpUDJyc-o-juHYSIdHwVdhlWSOxmvvq0kf0VITR5wPs5aw02GSUDt2FPxaw-wPFu-3_UrVWTJI0ZgI9D2zzuVDDTvipFn_huRTzPqJN6Q8uHT1dgfaQh86WS8ZqbHuL_oMAxhXLdRwtc07nAm-1lyV3ISjeIicqfli8PZ8IOUWDNQ Make sure you configure duration when you create the service account token.\nThat means the token will expire based on the duration you configured. For example, I\u0026rsquo;ve set the duration to 8760h (365 days) and created the service account token on November 25, 2024. That token will expire on November 25, 2025.\nDeploying Kubernetes Dashboard UI # Before we use the service account token, make sure you deployed the Kubernetes Dashboard, a general-purpose web UI for Kubernetes clusters. In this article, we will use the Kubernetes dashboard to manage and monitor the Kubernetes cluster.\nYou can use the Helm package manager to deploy the Kubernetes dashboard. Please, see https://github.com/kubernetes/dashboard/blob/master/README.md#installation.\nAdd the kubernetes-dashboard Helm repository.\n$ helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard Deploy the Kubernetes dashboard UI using the kubernetes-dashboard Helm chart.\n$ helm install kubernetes-dashboard \\ kubernetes-dashboard/kubernetes-dashboard \\ --create-namespace \\ --namespace kubernetes-dashboard Read more configuration details on the ArtifactHub, https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard\nUsing ServiceAccount Tokens #We have two options to log in to the Kubernetes dashboard to manage and monitor the Kubernetes cluster. You can also use the kubectl client tool to interact with the Kubernetes cluster.\nServiceAccount Token (Only) Kubeconfig with ServiceAccount Token ServiceAccount Token (Only) #Make sure you have deployed the Kubernetes dashboard UI on the Kubernete cluster. In the previous section, we have generated the service account token. We can simply use this token to log in to the Kubernetes dashboard.\nPreviously, a generated service account token:\n... status: expirationTimestamp: \u0026#34;2025-11-25T04:13:16Z\u0026#34; token: eyJhbGciOiJSUzI1NiIsImtpZCI6Ii04blMtX1BpVHBQUUNxbXMxSEI3bkxRU1ZfNkVIR09UZGNiazlaWEoyaHMifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiLCJrM3MiXSwiZXhwIjoxNzY0MDQzOTk2LCJpYXQiOjE3MzI1MDc5OTYsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJjaGFybGllIiwidWlkIjoiMDAxOTYyNDUtNzhiNy00NzFkLThjODAtYWFhYWMyM2I2NjJjIn19LCJuYmYiOjE3MzI1MDc5OTYsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpjaGFybGllIn0.PokSrN-g4vz-9cl9d7_KuVl3UGh5q2Slww0G68uE4hrocSvrFizmCch4VkKCrzZ7anp9P3nWBvk8WVizo4nN5U_Wn3p-h7_nsvyTrJ7_FflNYS6w8fbfFVKl0vD31MVrpmj40DApkbIepnDKzKfvvNsf00zFoNyi2iB7iIgsrkpUDJyc-o-juHYSIdHwVdhlWSOxmvvq0kf0VITR5wPs5aw02GSUDt2FPxaw-wPFu-3_UrVWTJI0ZgI9D2zzuVDDTvipFn_huRTzPqJN6Q8uHT1dgfaQh86WS8ZqbHuL_oMAxhXLdRwtc07nAm-1lyV3ISjeIicqfli8PZ8IOUWDNQ Go to the Kubernetes dashboard UI \u0026ndash;\u0026gt; Token \u0026ndash;\u0026gt; Enter token \u0026ndash;\u0026gt; Sign in\nThen, you will see system:serviceaccount:kube-system:charlie service account as a logged-in user in the profile.\nKubeconfig with ServiceAccount Token #When you\u0026rsquo;ve bootstrapped and setup a Kubernetes cluster, it sets a default kubeconfig file. But, file location depends on the Kubernetes distribution. For example, in the K3s Kubernetes distribution, the default kubeconfig file location is /etc/rancher/k3s/k3s.yaml.\nWe will create and clone a kubeconfig file named kubeconfig-charlie from the original K3s kubeconfig /etc/rancher/k3s/k3s.yaml file. Then, we will set and replace the default admin user with Charlie and its token. Then, Charlie can access the Kubernetes cluster as a read-only or view-only user using the kubeconfig file.\napiVersion: v1 kind: Config clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkekNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdGMyVnkKZG1WeUxXTmhRREUyTmpBd05qRTFNVFV3SGhjTk1qSXdPREE1TVRZeE1UVTFXaGNOTXpJd09EQTJNVFl4TVRVMQpXakFqTVNFd0h3WURWUVFEREJock0zTXRjMlZ5ZG1WeUxXTmhRREUyTmpBd05qRTFNVFV3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFSUW1wQnBRY3lhL0dUa2FPL2JLSGNHc2tac2M0UHpaNElYc3ZQVVQzMUoKNjNVb3AxQXZ4WlhObDhoRWk1ZkxFL2s1WC8zSnNreE5aSHJmSHVoY0lKODVvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVWlJUkN0QU50VEZPR3cycEZ1ZGxQCmhZWXIzQTR3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUloQUtPTUVNRjVmaUxkc0JxV0lpL0k5VnNJQS9BWEVLWEUKVXhSajJYQ2szUnQ4QWlBcDQ4SFZjRytodXIvS2hZaTVnSUZ3Vk1wUGYyWHV3WE1SRk5wV09kZEZ6Zz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K server: https://\u0026lt;k3s_server_address\u0026gt;:6443 name: k3s-dev-cluster contexts: - context: cluster: k3s-dev-cluster user: charlie name: k3s-dev-cluster current-context: k3s-dev-cluster preferences: {} - name: charlie user: token: eyJhbGciOiJSUzI1NiIsImtpZCI6Ii04blMtX1BpVHBQUUNxbXMxSEI3bkxRU1ZfNkVIR09UZGNiazlaWEoyaHMifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiLCJrM3MiXSwiZXhwIjoxNzY0MDQzOTk2LCJpYXQiOjE3MzI1MDc5OTYsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJjaGFybGllIiwidWlkIjoiMDAxOTYyNDUtNzhiNy00NzFkLThjODAtYWFhYWMyM2I2NjJjIn19LCJuYmYiOjE3MzI1MDc5OTYsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpjaGFybGllIn0.PokSrN-g4vz-9cl9d7_KuVl3UGh5q2Slww0G68uE4hrocSvrFizmCch4VkKCrzZ7anp9P3nWBvk8WVizo4nN5U_Wn3p-h7_nsvyTrJ7_FflNYS6w8fbfFVKl0vD31MVrpmj40DApkbIepnDKzKfvvNsf00zFoNyi2iB7iIgsrkpUDJyc-o-juHYSIdHwVdhlWSOxmvvq0kf0VITR5wPs5aw02GSUDt2FPxaw-wPFu-3_UrVWTJI0ZgI9D2zzuVDDTvipFn_huRTzPqJN6Q8uHT1dgfaQh86WS8ZqbHuL_oMAxhXLdRwtc07nAm-1lyV3ISjeIicqfli8PZ8IOUWDNQ Previously, we have created a service account named charlie and generated its token. We will use them in the kubeconfig file. Make sure you set the service account user name and its token in the kubeconfig-charlie file like this.\n... contexts: - context: cluster: k3s-dev-cluster user: charlie name: k3s-dev-cluster ... users: - name: charlie user: token: eyJhbGciOiJSUzI1NiIsImtpZCI6Ii04blMtX1BpVHBQUUNxbXMxSEI3bkxRU1ZfNkVIR09UZGNiazlaWEoyaHMifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiLCJrM3MiXSwiZXhwIjoxNzY0MDQzOTk2LCJpYXQiOjE3MzI1MDc5OTYsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJjaGFybGllIiwidWlkIjoiMDAxOTYyNDUtNzhiNy00NzFkLThjODAtYWFhYWMyM2I2NjJjIn19LCJuYmYiOjE3MzI1MDc5OTYsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpjaGFybGllIn0.PokSrN-g4vz-9cl9d7_KuVl3UGh5q2Slww0G68uE4hrocSvrFizmCch4VkKCrzZ7anp9P3nWBvk8WVizo4nN5U_Wn3p-h7_nsvyTrJ7_FflNYS6w8fbfFVKl0vD31MVrpmj40DApkbIepnDKzKfvvNsf00zFoNyi2iB7iIgsrkpUDJyc-o-juHYSIdHwVdhlWSOxmvvq0kf0VITR5wPs5aw02GSUDt2FPxaw-wPFu-3_UrVWTJI0ZgI9D2zzuVDDTvipFn_huRTzPqJN6Q8uHT1dgfaQh86WS8ZqbHuL_oMAxhXLdRwtc07nAm-1lyV3ISjeIicqfli8PZ8IOUWDNQ Then, you can access the Kubernetes cluster with this kubeconfig file using the Kubernetes dashboard or kubectl command-line tool.\nTo log in to the Kubernetes dashboard with the kubeconfig file, same as in the previous section, log in with the token.\nGo to the Kubernetes dashboard \u0026ndash;\u0026gt; Kubeconfig \u0026ndash;\u0026gt; Choose the kubeconfig file \u0026ndash;\u0026gt; Sign in\nDemo: Testing Configured RBAC Bindings #In this section, we will demonstrate whether service account tokens with RBAC role bindings are working or not. We can use both the Kubernetes dashboard UI and the kubectl command-line tool to interact with the Kubernetes cluster.\nListing Pods from All Namespaces #Firstly, we need to set the KUBECONFIG environment variable to the kubeconfig file if you want to use the kubectl command-line tool. For example,\n$ export KUBECONFIG=~/.kube/kubeconfig-charlie Then, we can verify the current context and cluster with kubectl.\n$ kubectl config view Output:\napiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://\u0026lt;k3s_server_address\u0026gt;:6443 name: k3s-dev-cluster contexts: - context: cluster: k3s-dev-cluster user: charlie name: k3s-dev-cluster current-context: k3s-dev-cluster kind: Config preferences: {} users: - name: charlie user: token: REDACTED We will list Pods from all namespaces with the kubectl tool. (Or) If you want to use the dashboard, go to the Kubernetes dashboard UI \u0026ndash;\u0026gt; Pods \u0026ndash;\u0026gt; Select All namespaces\n$ kubectl get pods --all-namespaces Output:\nNAMESPACE NAME READY STATUS RESTARTS AGE kube-system svclb-loki-stack-fccf98b8-g596j 1/1 Running 1 (15d ago) 24d kube-system svclb-coredns-infra-9b5a1b8e-t7zvr 2/2 Running 2 (15d ago) 141d kube-system svclb-chatwoot-6f5e1870-qs8nt 1/1 Running 1 (15d ago) 173d kube-system svclb-ingress-nginx-controller-9eb0868a-76x45 5/5 Running 5 (15d ago) 48d kube-system svclb-coredns-infra-9b5a1b8e-jllth 2/2 Running 2 (15d ago) 67d kube-system svclb-ingress-nginx-controller-9eb0868a-gnfzp 5/5 Running 5 (15d ago) 48d dev chatwoot-client-7c5464856d-p2xdd 1/1 Running 1 (15d ago) 67d kube-system svclb-chatwoot-6f5e1870-c2sfg 1/1 Running 1 (15d ago) 67d kube-system svclb-loki-stack-fccf98b8-8kdgf 1/1 Running 1 (15d ago) 24d kube-system svclb-kafka-infra-controller-0-external-8796a87b-rq7d8 1/1 Running 1 (15d ago) 67d monitoring-system kube-prometheus-stack-prometheus-node-exporter-zrlnh 1/1 Running 2 (15d ago) 67d openebs openebs-ndm-operator-7ddccf59c4-64664 1/1 Running 1 (15d ago) 56d monitoring-system kube-prometheus-stack-prometheus-node-exporter-rx4cp 1/1 Running 9 (15d ago) 471d argocd argocd-notifications-controller-f95dc686f-tprcj 1/1 Running 1 (15d ago) 56d argocd argocd-dex-server-9595746ff-h7rcp 1/1 Running 1 (15d ago) 56d kube-system svclb-loki-stack-fccf98b8-nz6hb 1/1 Running 1 (15d ago) 24d loki-stack loki-stack-promtail-sf52l 1/1 Running 1 (15d ago) 67d kube-system svclb-kafka-infra-controller-0-external-8796a87b-wdbrh 1/1 Running 2 (15d ago) 173d openebs openebs-localpv-provisioner-686b564b5d-7llqf 1/1 Running 1 (15d ago) 56d You can also list any other Kubernetes resources like Deployments, DemonSets, StatefulSets, ConfigMaps, Services, Ingresses, and so on. And note that we have also allowed to view Pods logs.\nDeleting or Restarting Pods #We will also test deleting or restarting the Pod resources with the kubectl client tool. (Or) If you want to use the dashboard, go to the Kubernetes dashboard UI \u0026ndash;\u0026gt; Pods \u0026ndash;\u0026gt; Select Namespace \u0026ndash;\u0026gt; Click any Pod \u0026ndash;\u0026gt; Delete resource\nFor example,\n$ kubectl delete pod kube-prometheus-stack-prometheus-node-exporter-qt44p --namespace monitoring-system Output:\nError from server (Forbidden): pods \u0026#34;kube-prometheus-stack-prometheus-node-exporter-qt44p\u0026#34; is forbidden: User \u0026#34;system:serviceaccount:kube-system:charlie\u0026#34; cannot delete resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;monitoring-system\u0026#34; You cannot delete or restart Pods because we are ONLY allowed to get, list and watch the API resources such as Pods, ConfigmMaps, Services, and so on in the default API group in the engineer ClusterRole that binds to the charlie user also known as service account.\nListing and Viewing Secrets #We will also test listing or viewing the Secret resources with the kubectl client tool. (Or) If you want to use the dashboard, go to the Kubernetes dashboard UI \u0026ndash;\u0026gt; Secrets \u0026ndash;\u0026gt; Select any namespace\nFor example,\n$ kubectl get secrets --all-namespaces Output:\nError from server (Forbidden): secrets is forbidden: User \u0026#34;system:serviceaccount:kube-system:charlie\u0026#34; cannot list resource \u0026#34;secrets\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope You cannot also list or view the Secret resources because we have not allowed to list the Secret resources in the default API group in the engineer ClusterRole that binds to the charlie user also known as service account. So, Charlie cannot list the Secret resources.\nFinally, we can confirm that the charlie ServiceAccount with ClusterRole and Role Bindings configuration are working properly.\n","date":"5 December 2024","permalink":"/blog/configure-k8s-auth-rbac/","section":"BLOG","summary":"","title":"Configuring RBAC Role Bindings for a User on Kubernetes"},{"content":"","date":null,"permalink":"/tags/rbac/","section":"TOPICS","summary":"","title":"Rbac"},{"content":"","date":null,"permalink":"/tags/android/","section":"TOPICS","summary":"","title":"Android"},{"content":"Android Devices အတွက် Kernel Source ကနေ Kernel တခု ဘယ်လို Build မလဲဆိုတဲ့ အကြောင်းအရာကို ဒီ How-To article မှာ အဓိက ပြောသွားမှာဖြစ်ပါတယ်။ Android OS က Linux Kernel ကို Based ထားပြီး Android ရဲ့ Kernel က Modified ထားတဲ့ Linux Kernel တခုပါ။ Android မှာသုံံံံံးထား Linux Kernel branch တွေက Long Term Support(LTS) branch တွေ ဖြစ်ပါတယ်။ https://www.kernel.org မှာ Long term branch တွေကြည့်နိုင်ပါတယ်။ ဥပမာ Nexus 5X, 6, 6P မှာဆိုရင် “linux-3.10-y” ဆိုတဲ့ branch ကို သုံးပါတယ် Google Pixel/Pixel XL မှာဆိုရင် “linux-3.18-y” ဆိုတဲ့ LTS branch တွေ သုံးကြပါတယ်။ Android OS က Linux Kernel ပေါ်မှာ အခြေခံပြီး တည်ဆောက်ထားတာ ဖြစ်ပြီး Kernel ဆိုတာ OS တခုရဲ့ အရေးကြီးတဲ့ အစိတ်အပိုင်းတခုပါ။ CPU, Memory, Disaply စတဲ့ Hardware အစိတ်အပိုင်းတွေ နဲ့ Software နဲ့ကြား ချိတ်ဆက်ပြီး အလုပ်လုပ်တဲ့ နေရာမှာ Kernel က အရေကြီးတဲ့ အပိုင်းမှာ ပါဝင်ပါတယ်။ Android OS Architecture ရဲ့ Linux Kernel အပိုင်းမှာ Display Driver, Camera Driver, USB Driver, Bluetooth Driver, Audio Driver, Power Management အစရှိသဖြင့်ပါဝင်ပါတယ်။ နမူနာပြောပြရရင် ကျွန်တော့််ရဲ့ Nexus 5X မှာ ပုံမှန် built-in ပါတဲ့ Stock Kernel မှာ Double Tap to Wake/Sleep / Disaply နဲ့ ပတ်သက်တဲ့ KCAL - Advanced Color Control / Audio driver နဲ့ ပတ်သက်တဲ့ Sound Control with High Performance Audio စသဖြင့် မပါဝင်ကြပါဘူး။ ကိုယ့်မှာ C Programming Skill ရှိရင် Kernel source တခု ကနေ အဲဒီ Kernel features တွေ ရေးပြီး ပြန် Recompile လုပ်နိုင်ပါတယ်။ ဒီနေရာမှာ Google ရဲ့ Nexus/Pixel လိုမျိုး Stock Pure Android ဖုန်းတွေ မဟုတ်တဲ့ တခြား Android OEMs တွေဖြစ်တဲ့ (Samsung, HTC, Sony and etc…) စတဲ့ Company တွေရဲ့ဖုန်းတွေမှာ တော်တော်များမှာ အဲဒီ Features အနည်းနဲ့အများ ပါဝင်ကြပါတယ်။ ဘာလု့ိ အဆင်သင့်ပါလဲဆိုတော့ သူတို့ရဲ့ Company က သက်ဆိုင်ရာ Android Engineer တွေက Source ကနေ Modified လုပ်ထားပြီးသားဖြစ်နေလု့ိပါပဲ။ အဲဒီ Features တွေ Device drivers - Audio, Display, Camera, USB and etc… / Memory / Power Management ပိုင်းတွေက Low-level ထိဆင်းပြီး C Programming နဲ့ရေးကြပါတယ်။ Custom Android Kernel တခု Build ရတဲ့အကြောင်းက Kernel source ယူပြီး Features တွေ ထပ်ပေါင်းထည့်ဖို့အတွက် ဖြစ်ပါတယ်။\nRequirements # GNU/Linux based Operating System(OS) Device\u0026rsquo;s Kernel Source Git: Version Control System GCC Toolchins (ARM/ARM64) Kernel Sources #Kernel source တွေက ဖုန်းအမျိုးအစာပေါ် မူတည်ပြီး download ရမယ့် site တွေက ကွဲပြားသွားပါလိမ့်မယ်၊ လိုအပ်တဲ့ Link တွေ အောက်မှပေးထားပါမယ်။\nGoogle Nexus/Pixel ( Qualcomm Chipset Only) : https://android.googlesource.com/kernel/msm Google Nexus (For all Chipsets) : https://android.googlesource.com/kernel/ Sony Xperia : https://developer.sonymobile.com/downloads/xperia-open-source-archives/ LG : http://opensource.lge.com/index Samsung : http://opensource.samsung.com/reception.do HTC : https://www.htcdev.com/devcenter/downloads Xiaomi : https://github.com/MiCode OnePlus : https://github.com/OnePlusOSS Motorola : https://github.com/MotorolaMobilityLLC နောက်တခုက အမျိုးမျိုးသော Android Device တွေရဲ့ Kernel source တွေ တနေရာတည်းမှာ ရနိုင်တဲ့ နေရကတော့ LineageOS ROM Community ကြီးပဲဖြစ်ပါတယ်။ (ဒါပေမယ့် တခုတော့ရှိတယ် အဲဒီ LineageOS Source ကနေ Build လိိုုက်တဲ့ Kernel တခုဟာ သူ့ရဲ့ ROM နဲ့ AOSP based ROM တွေမှာပဲ အလုပ်လုပ်ပါလိမ့်မယ်၊ ဥပမာ Xiaomi Device တွေ အနေနဲပြောရရင် သူ့ရဲ့ StockROM (MIUI) မှာ LineageOS source ကနေ build ထားတဲ့ Kernel ကို သုံးလို့ရမှာ မဟုတ်ပါဘူး အလုပ်လုပ်မှာ မဟုတ်ပါဘူး၊ ဖုန်းက LineageOS တင် ထားဖို့လိုပါယ်။ https://github.com/LineageOS Toolchains #Kernel Source ကနေ compile ဖို့အတွက်ဆိုရင် Toolchains တခုလိုအပ်ပါတယ်၊ Toolchains မှာ ကိုယ့်ဖုန် ရဲ့ CPU arch ပေါ် မူတည်ပြီ ARM နဲ့ ARM64 ဆိုပြီး ၂မျိုး ရှိပါတယ်။ လိုအပ်တဲ့ Link တွေ အောက်မှာ ပေးထားပါတယ်။\narm : https://android.googlesource.com/platform/prebuilts/gcc/linux-x86/arm/arm-eabi-4.8/ arm64 : https://android.googlesource.com/platform/prebuilts/gcc/linux-x86/aarch64/aarch64-linux-android-4.9/ Downloading the Kernel Sources #ဒီ TUT ကို ကျွန်တော်မှာရှိတဲ့ Nexus 5X နဲ့ ဥပမာပေးပြီး ပြောသွားမှာပါ။ ကျန်တဲ့ဖုန်းတွေ အတွက်ကလည်း သဘောတရာက တူတူပါပဲ၊ Kernel Source download တဲ့ နေရာပဲ ကွာသွားမှာပါ။\nအရင်ဆုံး Terminal ကိုဖွင့်ပြီး ဒီ command လေးရိုက်လိုက်ပါ။ (Dir တခုဆောက်ပါမယ်) mkdir KernelName cd KernelName Nexus 5X အတွက် Kernel source download ဖို့ အတွက် ဒီ command လေး ရိုက်လိုက်ပါ။ အရင်ဆုံး ကိုယ့် Computer ထဲမှာ git install ထားဖို့ လိုပါတယ်။ git clone -b android-msm-bullhead-3.10-oreo-r4 --depth=1 https://android.googlesource.com/kernel/msm ပြီးရင် Kernel compile ဖို့အတွက် Toolchains download ရပါမယ်။ (ဒီနေရာမှာ တခု သတိထားဖို့လိုပါတယ် ကိုယ်ရဲ့ဖုန်း CPU arch က arm64 ဆို arm64 toolchains ကို download ပါ၊ မဟုတ်ဘူး arm ဆိုရင် arm toolchains ကို download ပါ) git clone https://android.googlesource.com/platform/prebuilts/gcc/linux-x86/aarch64/aarch64-linux-android-4.9 အဲဒါတွေအကုန်ပြီးသွာပြီ ဆိုရင် Kernel build ဆို အဆင်သင့် ဖြစ်ပါပြီ။ Building the Kernel # အရင်ဆုံး Kernel source နဲ့ toochains ကို ပထမက ဆောက်ထားတဲ့ KernelName (PuerZ-Kernel-N5X) ဆိုတဲ့ Dir ထဲမှာ နှစ်ခုလုံး အဆင်သင့် ရှိနေရပါမယ်။ Toolchain Name ကို AOSP-Toolchains လို့ အမည်ပေးလိုက်ပြီး၊ Nexus 5X Kernel Source Name ကို bullhead လို့ အမည် ပေးလိုက်ပါမယ်။ (အဆင်ပြေသလို Rename လိုက်ပါ ပြဿ နာ မရှိပါဘူး၊ တခုပဲ Toolchains Location ပြန် ပေးတဲ့ နေရာမှာ အဲဒီ Name တွေအတိုင်း အတိအကျသိ ဖို့ လိုပါတယ်) e.g : Toolchains location /home/zawzaw/PureZ-Kernel-N5X/AOSP-Toolchains e.g : Kernel Source location /home/zawzaw/PureZ-Kernel-N5X/bullhead ပြီးရင် ကိုယ့်ဖုန်းအတွက် download ထားတဲ့ Kernel source Folder ထဲ ဝင်လိုက်ပါ။ Right Click ထောက်ပြီး Terminal လေးကို ဖွင့်လိုက်ပါ။ ပထမဦးဆုံး လုပ်ရမှာ Kernel source ကနေ compile ဖို့အတွက် export ဆိုတဲ့ command ကို သုံးပြီး toolchains ကို Set new environment variable သွားလုပ်ရပါမယ်။ (export - Set a New Environmetn Variable) Type this command (အဲဒီမှာ bin/နောက်ကကောင်ကို toochains prefix လို့ခေါ်ပါတယ် အခု Google က ပေးထားတဲ့ Toochain တွေ ရဲ့ prefix တွေကို ပြောပြပါမယ်၊ ARM အတွက်ဆိုရင် \u0026ldquo;arm-eabi-\u0026rdquo; ၊ ARM64 အတွက်ဆိုရင် \u0026ldquo;aarch64-linux-android-\u0026rdquo; ဖြစ်ပါတယ်) export CROSS_COMPILE=${HOME}/PureZ-Kernel-N5X/AOSP-Toolchains/bin/aarch64-linux-android- ကိုယ့်ဖုန်းရဲ့ CPU arch က arm လား arm64 လား သိထားဖို့ အရင်လိုပါတယ် အရင်ဆုံး ကိုယ့်ဖုန်းရဲ့ arch ကို ပြောပေးဖို့ လိုပါတယ်။ Nexus 5X က arm64 ဖြစ်တဲ့အတွက် ဒီ command လေး ဆက်ရိုက်လိုက်ပါ။ (တကယ်လို့ ကိုယ့်ဖုန်းက arm ဆိုရင် arm64 နေရာမှာ armလို့ ပြောင်း ရိုက်လိုက်ပါ။ export ARCH=arm64 \u0026amp;\u0026amp; export SUBARCH=arm64 နောက်တခုက Kernel source ထဲမှာ Compile ထား output file တွေ ရှိရင် ရှင်း ပေးဖို့ လိုပါတယ်။ make clean \u0026amp;\u0026amp; make mrproper နောက်ထက်တခု သိဖို့ကတော့ ကိုယ့် build မယ့် Kernel ရဲ့ build kernel configuration ပါ။ ARM device ဆိုရင် kernelsource/arch/arm/configs/ အောက်မှာ ရှိပါတယ်။ ARM64 device ဆိုရင် kernelsource/arch/arm64/configs/ အောက်မှာ ရှိပါတယ်။ Nexus 5X အတွက်ဆိုရင် bullhead/arch/arm64/configs/bullhead_defconfig (bullhead_defconfig ဆိုတာ Nexus 5X အတွက် build မယ့် kernel configuration အပိုင်းပါပဲ) ကိုယ့်ဖုန်းအတွက် kernel defconfig ကို သိချင်ရင် KernelSource/build.config file လေးကို ဖွင့်ကြည်နိုင်ပါတယ်။ အရင်ဆုံး Kernel compile မလုပ်ခင် build configuration လုပ်ပေးဖို့ လိုပါတယ်။ make bullhead_defconfig ပြီးရင် Kernel compile ပါတော့မယ်၊ compile ဖို့အတွက် အောက်က command လေးရိုက်လိုက်ပါ။ make -j$(nproc --all) Compilation process time က ကိုယ့် Computer ရဲ့ CPU core ပေါ်မူတည်ပြီးကြာနိုင်ပါတယ်။\nအဲဒါတွေပြီးသွားရင် Compiler ကနေ Compile လုပ်သွားပါလိမ့်မယ်။ Build လိုက်တဲ့ Kernel zImage တွေက ARM ဆိုရင် - kernelsource/arch/arm/boot/အောက်မှာ ထွက်ပါတယ်၊ ARM64 ဆိုရင် - kernelsource/arch/arm64/boot/အောက်မှာ ထွက်သွားလိမ့်မယ်။ အဲဒါ တွေ အောင်မြင်သွာပြီး ဆိုရင် ကိုယ်ဖုန်းအတွက် Kernel Install ဖု့ိ FlashableZip ဘယ်လိုလုပ်မလဲ ဆိုတာ ဆက်ရေးပါမယ်။\n","date":"9 March 2018","permalink":"/blog/build-android-kernel/","section":"BLOG","summary":"","title":"Building Kernel For Android Devices"},{"content":" Zaw Zaw I\u0026rsquo;m Zaw Zaw from Pyay, Myanmar and an SRE/Platform Engineer. Full name is Zaw Zaw Thein. I’m publishing articles that focus on Linux, containerization, Container networking, Kubernetes, and Cloud-native technologies, and also writing about simple living, personal development \u0026amp; growth, and experience on the blog.\nI\u0026rsquo;m a Former Recognized Developer (RD) at @XDA-Developers Forums. I’ve previously contributed to the Android operating system, Linux kernel, and Android Open Source Project (AOSP) based Android custom firmware projects on XDA Android Forums. I’ve loved working with Low-level software and systems programming.\nCurrently, I’m working on Site Reliability Engineering (SRE), Linux, CI/CD tools, Containerization, and Kubernetes. I’ve always had a keen interest in Computer engineering, Systems programming, Linux kernel, Cloud computing, Containerization, Cloud-native technologies and Kubernetes. And I also love to take photographs and am interested in photography.\nEDUCATION # Studied Computer Engineering at Pyay Technological University (P.T.U) Graduated from Basic Education High School (Sinmizwe) PROFESSIONAL SKILLS # Programming \u0026amp; Scripting: C/C++, Java, Bash Shell Leadership: Team Management, Mentorship, SRE Practices Operating Systems: Linux (Debian, Ubuntu, Fedora, RHEL) Cloud Platforms: AWS (EC2, EKS, S3) Version Control System: Git Continuous Integration (CI): GitLab CI, GitHub Actions Continuous Delivery (CD): Argo CD, Flux CD Networking: TCP/IP, DNS, Load Balancing, Linux Virtual Networking, Ingress NGINX, Istio Service Mesh Databases: MongoDB, MySQL, PostgreSQL Containerization \u0026amp; Orchestration: Docker, Podman, Kubernetes IaC \u0026amp; Configuration Management: Terraform (Basic), Ansible, Kustomize, Helm Monitoring and Observability: Prometheus, Grafana, Loki, Jaeger, ELK Stack (ElasticSearch, Logstash, Kibana) WORK EXPERIENCE #Frontiir # Lead Platform Engineer ‣ Apr 2024 - Apr 2025\nLed the Platform Engineering team. Led the building and migrating of GitOps ArgoCD pipelines for automating app deployments for both development and production environments on Kubernetes. Maintained self-managed internal K3s Kubernetes Clusters. Migrated legacy apps to Kubernetes and Cloud-native environments. Mentored Junior Platform engineers in GitLab CI, GitOps ArgoCD, and Kubernetes, focused on team building and researched new tools. Platform Engineer ‣ May 2023 - Mar 2024\nDesigned and created multi-node K3s Kubernetes Clusters. Integrated APISIX API Gateway and Keycloak for the microservices project on Kubernetes. Configured GitLab Runners on Kubernetes for internal OSS and BSS projects and maintained them. Developed Ansible Playbooks to bootstrap the Kubernetes Nodes. Configured and deployed Prometheus and Grafana for monitoring database servers and Kubernetes clusters. Senior Associate SRE Engineer ‣ Nov 2021 – Apr 2023\nContainerized OSS Software systems and built Helm charts to deploy them on Kubernetes. Configured GitOps ArgoCD Pipelines for automating OSS microservices app deployments on Kubernetes. Migrated and configured the Istio Service Mesh for the OSS microservices project. Built an operator for automating updating app container images on Kubernetes. Associate SRE Engineer ‣ Sep 2020 – Oct 2021\nMoved to the SRE team. Containerized apps and developed GitLab CI pipelines. Associate System Engineer ‣ Jan 2019 - Aug 2020\nWorked on building, porting and customizing the Linux kernel and embedded Android operating system (OS) for AMLogic SoC-based Android TV hardware devices. XDA-Developers # Recognized Developer ‣ 2017 - 2019\nContributed to Android, Linux kernel and Android open-source project, AOSP-based Android firmware projects as Recognized Developer and Contributor on XDA Community Forums. Senior Member ‣ 2016 - 2017\nContributed to some Android firmware and MOD projects as Senior Member on XDA Community Forums. INTERESTS # Computer Engineering Linux kernel Systems Programming Operating Systems SRE (Site Reliability Engineering) Cloud Computing Cloud-native Technologies Virtualization Containerization Photography Sharing, Reading and Writing ","date":null,"permalink":"/about/","section":"Welcome to ZawZaw.blog","summary":"","title":"ABOUT"},{"content":" Comprehensive and Hands-on Guides ","date":null,"permalink":"/guides/","section":"GUIDES","summary":"","title":"GUIDES"},{"content":" Photos about Macro, City, Nature, and Landscape Photography I am also interested in photography and I love to take photographs as an Amateur Photographer. Most photos are about Macro, City, Nature, and Landscape photography, and are taken with the Google Nexus and Pixel devices.\nYou can see the photos on the following Pixieset Gallery website, Google Photos Album, and Instagram.\nPixieset: https://thezawzaw.pixieset.com\nGoogle Photos: https://photos.app.goo.gl/SJ9NYCk8so8oJRQz7\nInstagram: https://www.instagram.com/thezawzaw\n","date":null,"permalink":"/photography/","section":"Welcome to ZawZaw.blog","summary":"","title":"PHOTOGRAPHY"}]